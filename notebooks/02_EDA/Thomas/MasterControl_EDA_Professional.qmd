---
title: "The Signal in the Noise: MasterControl Strategic EDA"
subtitle: "A Masterclass in Identifying the Ideal Customer Profile (ICP) for Mx Expansion"
author: "MSBA Capstone Group 3"
date: "Spring 2026"
format:
  html:
    theme: journal
    toc: true
    toc-depth: 3
    toc-float: true
    number-sections: false
    code-fold: true
    code-tools: true
    df-print: paged
    highlight-style: github
  pdf:
    documentclass: article
    geometry:
      - margin=1in
    toc: true
    number-sections: false
    colorlinks: true
    mainfont: "Arial"
    sansfont: "Arial"
    monofont: "Courier New"
editor: visual
---

# The Business Intelligence Briefing

## 1.1 The Client: MasterControl

Before analyzing a single row of data, we must understand the engine we are tuning. **MasterControl** is a SaaS provider for the **Life Sciences** industry (Pharmaceuticals, Biotech, and Medical Devices).

* **The Stakes:** These industries are highly regulated by the FDA. One mistake in a manufacturing record can lead to millions in fines or a plant shutdown. MasterControl sells software that digitizes these records to ensure compliance and efficiency.
* **The Landscape:** They are not selling generic software; they are selling *safety and speed* to people who make life-saving drugs and devices.

## 1.2 The "Tale of Two Products" (The Core Problem)

This case is defined by the performance gap between two product lines:

1.  **Qx (Quality Excellence):**
    * *The Legacy:* This is the "Cash Cow." MasterControl has sold QMS (Quality Management Systems) for 20+ years.
    * *The Buyer:* Quality Managers (Risk-averse, compliance-focused).
    * *Performance:* **19.7% Conversion Rate.** The sales motion is a well-oiled machine.

2.  **Mx (Manufacturing Excellence):**
    * *The Challenger:* A newer product (launched ~4 years ago) focused on MES (Manufacturing Execution Systems) ‚Äì digitizing the shop floor.
    * *The Buyer:* Operations/Plant Directors (Efficiency-focused).
    * *The Problem:* **12.7% Conversion Rate.** It is underperforming Qx by **~55%**.

**Our Mission:** The data suggests MasterControl is trying to sell Mx to the *wrong people* (Quality Managers) at the *wrong companies*. We must identify the **Ideal Customer Profile (ICP)**‚Äîthe specific combination of **Job Title**, **Industry**, and **Manufacturing Model**‚Äîthat actually buys Mx.

---

# System Architecture

We begin by initializing a "Competition-Grade" environment. We use a specific color palette (`#00534B` Teal vs `#F05627` Orange) to visually reinforce the "Success vs Failure" binary in every chart.

```{python}
#| label: setup
#| warning: false
#| message: false

# ==============================================================================
# CORE ARCHITECTURE
# ==============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from pathlib import Path
from scipy import stats
from scipy.stats import beta, mannwhitneyu
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.tree import DecisionTreeClassifier, plot_tree
import warnings

warnings.filterwarnings('ignore')

# --- THE PSYCHOLOGICAL PALETTE ---
# In our visuals, Teal always represents the GOAL (Success/Mx).
# Orange always represents the PROBLEM (Failure/Gap/Qx Benchmark).
PROJECT_COLS = {
    'Success': '#00534B',  # MasterControl Teal
    'Failure': '#F05627',  # Risk Orange
    'Neutral': '#95a5a6',  # Gray for noise
    'Mx': '#00534B',       # Focus Product
    'Qx': '#F05627',       # Benchmark Product
    'Highlight': '#2980b9' # Blue for emphasis
}

# Statistical thresholds
ALPHA = 0.05
MIN_SAMPLE_SIZE = 30
BAYESIAN_PRIOR_ALPHA = 1
BAYESIAN_PRIOR_BETA = 1

# Plotting configuration
sns.set_theme(style="whitegrid", context="talk")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.titleweight'] = 'bold'

print("‚úì System Initialized: Data Science Environment Active")
print(f"‚úì Visual Strategy: Success={PROJECT_COLS['Success']} | Failure={PROJECT_COLS['Failure']}")
```

---

# Feature Engineering (The Secret Weapon)

## 3.1 The Data Asset

We are analyzing **16,644 Qualified Leads (QALs)** from 2024-2025.

* **Target Variable:** `next_stage__c`. Did the lead die ("Recycled") or progress to a sales opportunity ("SQL/SQO/Won")?
* **The Hidden Gold:** The `contact/lead title` column contains 4,000+ unstructured text strings (e.g., "Sr. Mgr of Quality & Ops").

## 3.2 The Engineering Strategy

Most teams will simply drop the messy text columns. **We will win by mining them.**
We define custom functions to extract structured signals from the chaos:

1. **The Authority Parser:** Extracts *Seniority* (VP vs. Manager). Hypothesis: Mx requires higher budget authority than Qx, so VPs should convert better.
2. **The Domain Parser:** Extracts *Function* (Quality vs. Ops). Hypothesis: "Operations" titles will drive Mx success, while "Quality" titles drive Qx success.
3. **The Scope Parser:** Extracts *Power Modifiers* (Global vs. Site). Hypothesis: "Global" roles have more budget authority.

```{python}
#| label: data-pipeline

# ==============================================================================
# THE DATA PIPELINE
# ==============================================================================

def bayesian_conversion_rate(successes, n, alpha=BAYESIAN_PRIOR_ALPHA, beta_param=BAYESIAN_PRIOR_BETA):
    """
    Bayesian credible interval using Beta-Binomial conjugacy.
    Returns: (posterior_mean, ci_low, ci_high)
    """
    post_alpha = alpha + successes
    post_beta = beta_param + (n - successes)

    mean = post_alpha / (post_alpha + post_beta)
    ci_low = beta.ppf(0.025, post_alpha, post_beta)
    ci_high = beta.ppf(0.975, post_alpha, post_beta)

    return mean, ci_low, ci_high


def clean_and_engineer(filepath=None):
    """
    Transforms raw CRM data into high-octane signals.
    """
    # 1. ROBUST LOADING --------------------------------------------------------
    if filepath is None:
        possible_paths = [
            Path.cwd() / "data" / "QAL Performance for MSBA.csv",
            Path.cwd().parent / "data" / "QAL Performance for MSBA.csv",
            Path.cwd().parent.parent / "data" / "QAL Performance for MSBA.csv",
            Path.cwd().parent.parent.parent / "data" / "QAL Performance for MSBA.csv"
        ]
        for p in possible_paths:
            if p.exists():
                filepath = p
                break

    if filepath is None:
        filepath = Path("QAL Performance for MSBA.csv")

    if not filepath.exists():
         print("Warning: Data file not found. Please ensure 'QAL Performance for MSBA.csv' is in the data folder.")
         return pd.DataFrame()

    df = pd.read_csv(filepath)

    # Standardize Headers
    df.columns = [c.strip().lower().replace(' ', '_').replace('/', '_').replace('-', '_')
                  for c in df.columns]

    # 2. TARGET DEFINITION -----------------------------------------------------
    success_stages = ['SQL', 'SQO', 'Won']
    df['is_success'] = df['next_stage__c'].isin(success_stages).astype(int)

    # Outcome Tiers (for Near-Miss Analysis)
    def classify_outcome(stage):
        if stage in ['SQL', 'SQO', 'Won']:
            return 'Success'
        elif stage == 'Recycled':
            return 'Near-Miss'
        else:
            return 'Lost'
    df['outcome_tier'] = df['next_stage__c'].apply(classify_outcome)

    # 3. PRODUCT SEGMENTATION --------------------------------------------------
    def segment_product(sol):
        if str(sol) == 'Mx': return 'Mx'
        elif str(sol) == 'Qx': return 'Qx'
        return 'Other'
    df['product_segment'] = df['solution_rollup'].apply(segment_product)

    # 4. TITLE PARSING (The Alpha Feature) -------------------------------------
    def parse_seniority(t):
        if pd.isna(t): return 'Unknown'
        t = str(t).lower()
        if re.search(r'\b(ceo|cfo|coo|cto|cio|chief|c-level|president|founder|owner)\b', t): return 'C-Suite'
        if re.search(r'\b(svp|senior vice president|evp)\b', t): return 'SVP'
        if re.search(r'\b(vp|vice president|head of)\b', t): return 'VP/Head'
        if re.search(r'\b(director)\b', t): return 'Director'
        if re.search(r'\b(manager|mgr|lead|supervisor)\b', t): return 'Manager'
        if re.search(r'\b(analyst|engineer|specialist|associate|coordinator)\b', t): return 'IC'
        return 'Other'

    def parse_function(t):
        if pd.isna(t): return 'Unknown'
        t = str(t).lower()
        if re.search(r'\b(manuf|prod|ops|plant|supply|site)\b', t): return 'Manufacturing/Ops'
        if re.search(r'\b(quality|qa|qc|qms|compliance|validation|capa)\b', t): return 'Quality/Reg'
        if re.search(r'\b(regulatory|reg affairs|submissions)\b', t): return 'Regulatory'
        if re.search(r'\b(it|info|sys|tech|data|soft)\b', t): return 'IT/Systems'
        if re.search(r'\b(lab|r&d|sci|dev|clin|research)\b', t): return 'R&D/Lab'
        if re.search(r'\b(project|program|pmo)\b', t): return 'PMO'
        return 'Other'

    def parse_scope(t):
        if pd.isna(t): return 'Standard'
        t = str(t).lower()
        if re.search(r'\b(global|worldwide|international|corporate|enterprise|group)\b', t): return 'Global'
        if re.search(r'\b(regional|division)\b', t): return 'Regional'
        if re.search(r'\b(site|plant|facility|local)\b', t): return 'Site'
        return 'Standard'

    df['title_seniority'] = df['contact_lead_title'].apply(parse_seniority)
    df['title_function'] = df['contact_lead_title'].apply(parse_function)
    df['title_scope'] = df['contact_lead_title'].apply(parse_scope)
    df['is_decision_maker'] = df['title_seniority'].isin(['C-Suite', 'SVP', 'VP/Head', 'Director']).astype(int)

    # 5. RECORD COMPLETENESS (Buyer Seriousness Proxy) -------------------------
    completeness_cols = ['acct_manufacturing_model', 'acct_primary_site_function',
                         'acct_target_industry', 'acct_territory_rollup', 'acct_tier_rollup']

    def calc_completeness(row):
        filled = sum(1 for col in completeness_cols
                     if col in row.index and pd.notna(row[col]) and str(row[col]) != 'Unknown')
        return filled / len(completeness_cols)

    df['record_completeness'] = df.apply(calc_completeness, axis=1)
    df['completeness_tier'] = pd.cut(df['record_completeness'],
                                      bins=[-0.01, 0.4, 0.8, 1.01],
                                      labels=['Low', 'Medium', 'High'])

    # 6. TEMPORAL FEATURES -----------------------------------------------------
    df['cohort_date'] = pd.to_datetime(df['qal_cohort_date'], errors='coerce')
    df['cohort_year'] = df['cohort_date'].dt.year
    df['cohort_quarter'] = df['cohort_date'].dt.to_period('Q').astype(str)
    max_date = df['cohort_date'].max()
    df['lead_age_days'] = (max_date - df['cohort_date']).dt.days

    # 7. STRATEGIC IMPUTATION --------------------------------------------------
    cols_to_fill = ['acct_manufacturing_model', 'acct_primary_site_function',
                    'acct_target_industry', 'acct_territory_rollup']
    for c in cols_to_fill:
        if c in df.columns:
            df[c] = df[c].fillna('Unknown')

    print("=" * 70)
    print("DATA PIPELINE COMPLETE")
    print("=" * 70)
    print(f"‚úì Total Rows: {len(df):,}")
    print(f"‚úì Target Rate (is_success): {df['is_success'].mean():.1%}")
    print(f"‚úì Mx Leads: {len(df[df['product_segment']=='Mx']):,}")
    print(f"‚úì Qx Leads: {len(df[df['product_segment']=='Qx']):,}")
    print(f"‚úì Near-Miss (Recycled): {len(df[df['outcome_tier']=='Near-Miss']):,}")
    print(f"‚úì Decision Makers: {df['is_decision_maker'].sum():,} ({df['is_decision_maker'].mean():.1%})")
    print("=" * 70)

    return df

# Execute Pipeline
df = clean_and_engineer()
```

---

# The Yield Gap Analysis

## 4.1 Quantifying the Pain

Before providing a solution, we must quantify the problem. The chart below is the "Money Slide." It proves that for every 100 leads:

* **Qx** generates ~20 opportunities.
* **Mx** generates ~12 opportunities.

That gap represents millions in wasted marketing spend and lost revenue.

```{python}
#| label: gap-analysis
#| fig-cap: "The Yield Gap: Mx trails Qx by ~7 percentage points. Closing this gap is the primary objective."

def plot_performance_gap(df):
    df_main = df[df['product_segment'].isin(['Mx', 'Qx'])]

    if len(df_main) == 0:
        return

    # Calculate stats with Bayesian CIs
    results = []
    for product in ['Mx', 'Qx']:
        subset = df_main[df_main['product_segment'] == product]
        n = len(subset)
        successes = subset['is_success'].sum()
        rate, ci_low, ci_high = bayesian_conversion_rate(successes, n)
        results.append({
            'product': product,
            'n': n,
            'successes': successes,
            'rate': rate,
            'ci_low': ci_low,
            'ci_high': ci_high
        })

    results_df = pd.DataFrame(results)

    # Create visualization
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # LEFT: Bar chart with CIs
    ax1 = axes[0]
    colors = [PROJECT_COLS['Mx'], PROJECT_COLS['Qx']]
    bars = ax1.bar(results_df['product'], results_df['rate'], color=colors,
                   edgecolor='white', linewidth=2, width=0.6)

    for i, row in results_df.iterrows():
        ax1.errorbar(i, row['rate'],
                    yerr=[[row['rate'] - row['ci_low']], [row['ci_high'] - row['rate']]],
                    fmt='none', color='black', capsize=12, capthick=2, linewidth=2)
        ax1.text(i, row['rate'] + 0.025, f"{row['rate']:.1%}",
                ha='center', va='bottom', fontsize=18, fontweight='bold')
        ax1.text(i, row['ci_low'] - 0.015, f"n={row['n']:,}",
                ha='center', va='top', fontsize=11, color='gray')

    ax1.set_ylabel('Conversion Rate (Lead ‚Üí SQL+)', fontsize=13)
    ax1.set_title('The Yield Gap: Mx vs Qx Performance\n(with 95% Bayesian Credible Intervals)',
                  fontweight='bold', fontsize=15)
    ax1.set_ylim(0, 0.30)
    ax1.axhline(y=df_main['is_success'].mean(), color='gray', linestyle='--', alpha=0.5)
    ax1.grid(axis='y', alpha=0.3)

    # RIGHT: Waterfall showing the gap
    ax2 = axes[1]
    mx_rate = results_df[results_df['product']=='Mx']['rate'].values[0]
    qx_rate = results_df[results_df['product']=='Qx']['rate'].values[0]
    gap = qx_rate - mx_rate

    waterfall_data = pd.DataFrame({
        'label': ['Qx Baseline', 'Performance Gap', 'Mx Current'],
        'value': [qx_rate, -gap, mx_rate],
        'color': [PROJECT_COLS['Qx'], PROJECT_COLS['Failure'], PROJECT_COLS['Mx']]
    })

    ax2.bar(waterfall_data['label'], waterfall_data['value'], color=waterfall_data['color'])
    ax2.axhline(y=0, color='black', linewidth=0.5)
    ax2.set_ylabel('Conversion Rate', fontsize=13)
    ax2.set_title(f'The {gap:.1%} Performance Gap\n(Revenue Opportunity)', fontweight='bold', fontsize=15)
    ax2.set_ylim(0, 0.25)

    # Add annotation
    ax2.annotate(f'GAP:\n{gap:.1%}', xy=(1, 0.05), fontsize=14, fontweight='bold',
                ha='center', va='bottom', color='white',
                bbox=dict(boxstyle='round', facecolor=PROJECT_COLS['Failure'], alpha=0.8))

    plt.tight_layout()
    plt.show()

    print(f"\n‚ö†Ô∏è  PERFORMANCE GAP: {gap:.1%} points")
    print(f"   Mx: {mx_rate:.1%} | Qx: {qx_rate:.1%}")
    print(f"   Mx performs {(1 - mx_rate/qx_rate)*100:.0f}% worse relative to Qx")

if not df.empty:
    plot_performance_gap(df)
```

## 4.2 The Funnel Visualization

Let's see where leads are falling off in the pipeline.

```{python}
#| label: funnel-viz
#| fig-cap: "The Pipeline Funnel: Visualizing where leads go in the sales process."

def plot_funnel_analysis(df):
    df_products = df[df['product_segment'].isin(['Mx', 'Qx'])]

    if len(df_products) == 0:
        return

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # LEFT: Stage distribution by product
    ax1 = axes[0]
    stage_dist = df_products.groupby(['product_segment', 'next_stage__c']).size().unstack(fill_value=0)
    stage_dist_pct = stage_dist.div(stage_dist.sum(axis=1), axis=0) * 100

    stage_dist_pct.plot(kind='bar', stacked=True, ax=ax1,
                        colormap='RdYlGn', edgecolor='white', linewidth=0.5)
    ax1.set_ylabel('Percentage of Leads', fontsize=12)
    ax1.set_xlabel('')
    ax1.set_title('Lead Outcome Distribution by Product', fontweight='bold')
    ax1.legend(title='Stage', bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)
    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)

    # RIGHT: Outcome Tier comparison (simplified view)
    ax2 = axes[1]
    outcome_dist = df_products.groupby(['product_segment', 'outcome_tier']).size().unstack(fill_value=0)
    outcome_dist = outcome_dist[['Lost', 'Near-Miss', 'Success']]  # Order matters
    outcome_dist_pct = outcome_dist.div(outcome_dist.sum(axis=1), axis=0) * 100

    x = np.arange(len(outcome_dist_pct.index))
    width = 0.25

    colors = [PROJECT_COLS['Failure'], PROJECT_COLS['Neutral'], PROJECT_COLS['Success']]

    for i, (col, color) in enumerate(zip(outcome_dist_pct.columns, colors)):
        ax2.bar(x + i*width, outcome_dist_pct[col], width, label=col, color=color)
        for j, val in enumerate(outcome_dist_pct[col]):
            ax2.text(x[j] + i*width, val + 1, f'{val:.0f}%', ha='center', fontsize=10, fontweight='bold')

    ax2.set_ylabel('Percentage', fontsize=12)
    ax2.set_xticks(x + width)
    ax2.set_xticklabels(outcome_dist_pct.index)
    ax2.set_title('Outcome Tiers: Success vs Near-Miss vs Lost', fontweight='bold')
    ax2.legend()
    ax2.set_ylim(0, 100)

    plt.tight_layout()
    plt.show()

if not df.empty:
    plot_funnel_analysis(df)
```

---

# Signal Detection (Finding the Solution)

## 5.1 Semantic Log-Odds (Decoding the "Buyer's Language")

We use a technique called **Log-Odds Ratio Analysis**.

* **Concept:** We treat job titles as a "Bag of Words." We count how often "Manager" appears in successful leads vs. failed leads.
* **The Math:** If "Director" appears 5x more often in successful Mx leads than failed ones, it has a high positive log-odds.
* **The Insight:** This tells us *exactly* which keywords sales reps should search for on LinkedIn.

```{python}
#| label: nlp-signal
#| fig-cap: "The Language of Buyers: Words on the right (Green) predict success. Words on the left (Orange) predict failure."

def semantic_log_odds(df, product_filter='Mx'):
    subset = df[df['product_segment'] == product_filter].copy()

    if len(subset) < 50:
        return None

    # Vectorize with bigrams for context
    vec = CountVectorizer(stop_words='english', min_df=5, ngram_range=(1,2),
                          token_pattern=r'\b[a-zA-Z]{2,}\b')
    X = vec.fit_transform(subset['contact_lead_title'].fillna(''))
    words = np.array(vec.get_feature_names_out())

    # Calculate frequencies with Laplace smoothing
    y = subset['is_success'].values
    x_pos = np.array(X[y==1].sum(axis=0)).flatten() + 1
    x_neg = np.array(X[y==0].sum(axis=0)).flatten() + 1

    # Log-Odds calculation
    p_pos = x_pos / x_pos.sum()
    p_neg = x_neg / x_neg.sum()
    log_odds = np.log(p_pos / p_neg)

    res = pd.DataFrame({
        'word': words,
        'log_odds': log_odds,
        'freq': x_pos + x_neg - 2,  # Remove smoothing for display
        'n_words': [len(w.split()) for w in words]
    })

    # Filter for significant volume
    res = res[res['freq'] > 25]

    top_signals = pd.concat([res.nlargest(12, 'log_odds'), res.nsmallest(12, 'log_odds')])
    top_signals = top_signals.sort_values('log_odds', ascending=True)

    # Visualization
    fig, ax = plt.subplots(figsize=(14, 10))

    colors = [PROJECT_COLS['Success'] if x > 0 else PROJECT_COLS['Failure']
              for x in top_signals['log_odds']]

    y_pos = range(len(top_signals))
    bars = ax.barh(y_pos, top_signals['log_odds'], color=colors, edgecolor='white')

    # Add labels
    for i, (idx, row) in enumerate(top_signals.iterrows()):
        label = f"{row['word']} (n={int(row['freq'])})"
        x_pos = 0.02 if row['log_odds'] > 0 else -0.02
        ha = 'left' if row['log_odds'] > 0 else 'right'
        ax.text(x_pos, i, label, va='center', ha=ha, fontsize=10, fontweight='bold')

    ax.axvline(0, color='black', linewidth=1.5)
    ax.set_yticks([])
    ax.set_xlabel("Log-Odds Ratio (‚Üí Higher Conversion | ‚Üê Lower Conversion)", fontsize=12)
    ax.set_title(f"Semantic Signals for {product_filter}: The 'Deal Maker' vs. 'Deal Killer' Keywords",
                 fontsize=16, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)

    # Add legend box
    ax.text(0.98, 0.98, 'GREEN = Call These\nORANGE = Avoid These',
            transform=ax.transAxes, fontsize=11, va='top', ha='right',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))

    plt.tight_layout()
    plt.show()

    print("\nüìû CALL LIST (Top phrases predicting success):")
    print(res.nlargest(10, 'log_odds')[['word', 'log_odds', 'freq']].to_string(index=False))

    print("\n‚õî AVOID LIST (Phrases predicting failure):")
    print(res.nsmallest(10, 'log_odds')[['word', 'log_odds', 'freq']].to_string(index=False))

    return res

if not df.empty:
    semantic_results = semantic_log_odds(df, 'Mx')
```

## 5.2 The "Magic Quadrant" (Micro-Segment Discovery)

A single variable is rarely the answer. We need to find the **Interaction Effects**.
This heatmap intersects **Industry** (Rows) with **Manufacturing Model** (Columns).

* **Green Cells:** These are our "Honey Pots." The specific combinations where Mx conversion skyrockets.
* **Action:** Marketing should redirect budget *exclusively* to these green zones.

```{python}
#| label: magic-quadrant
#| fig-cap: "The Magic Quadrant: Identifying high-yield micro-segments for Mx."

def interaction_heatmap(df, product='Mx'):
    subset = df[df['product_segment'] == product]

    if len(subset) < 50:
        return

    # Pivot tables
    pivot_rate = subset.pivot_table(index='acct_target_industry',
                                    columns='acct_manufacturing_model',
                                    values='is_success',
                                    aggfunc='mean')

    pivot_n = subset.pivot_table(index='acct_target_industry',
                                 columns='acct_manufacturing_model',
                                 values='is_success',
                                 aggfunc='count')

    # Create annotation with rate and n
    annot = pivot_rate.copy().astype(str)
    for i in pivot_rate.index:
        for j in pivot_rate.columns:
            rate = pivot_rate.loc[i, j]
            n = pivot_n.loc[i, j] if pd.notna(pivot_n.loc[i, j]) else 0
            if pd.notna(rate) and n >= 15:
                annot.loc[i, j] = f'{rate:.0%}\n(n={int(n)})'
            else:
                annot.loc[i, j] = ''

    # Mask cells with low sample size
    mask = pivot_n < 15

    plt.figure(figsize=(14, 10))
    sns.heatmap(pivot_rate, annot=annot, fmt='', mask=mask,
                cmap="RdYlGn", center=subset['is_success'].mean(),
                cbar_kws={'label': 'Conversion Rate'}, linewidths=0.5)
    plt.title(f"The 'Magic Quadrant' for {product}: Where do we Win?\n(Gray = Insufficient Data, n<15)",
              fontsize=16, fontweight='bold')
    plt.ylabel("Industry Sector", fontsize=12)
    plt.xlabel("Manufacturing Model", fontsize=12)
    plt.tight_layout()
    plt.show()

    # Find top segments
    pivot_flat = pivot_rate.stack().reset_index()
    pivot_flat.columns = ['Industry', 'Model', 'Rate']
    pivot_flat_n = pivot_n.stack().reset_index()
    pivot_flat_n.columns = ['Industry', 'Model', 'N']
    pivot_flat = pivot_flat.merge(pivot_flat_n)
    pivot_flat = pivot_flat[pivot_flat['N'] >= 15].sort_values('Rate', ascending=False)

    print(f"\nüéØ TOP 5 SEGMENTS for {product} (n >= 15):")
    print(pivot_flat.head(5).to_string(index=False))

if not df.empty:
    interaction_heatmap(df, 'Mx')
```

## 5.3 The "Power Trio" Interaction

Does **Seniority** matter more in **Operations** than in **Quality**?

```{python}
#| label: power-trio
#| fig-cap: "The Power Trio: Function x Seniority interaction reveals the ICP."

def plot_power_trio(df, product='Mx'):
    subset = df[df['product_segment'] == product].copy()

    # Filter to main categories
    subset = subset[subset['title_function'].isin(['Manufacturing/Ops', 'Quality/Reg', 'IT/Systems', 'R&D/Lab'])]
    subset = subset[subset['title_seniority'].isin(['VP/Head', 'Director', 'Manager', 'IC'])]

    if len(subset) < 50:
        return

    # Calculate rates with sample sizes
    viz = subset.groupby(['title_function', 'title_seniority']).agg(
        rate=('is_success', 'mean'),
        n=('is_success', 'size')
    ).reset_index()

    # Filter for minimum sample size
    viz = viz[viz['n'] >= 10]

    fig, axes = plt.subplots(1, 2, figsize=(16, 7))

    # LEFT: Grouped bar chart
    ax1 = axes[0]
    pivot_viz = viz.pivot(index='title_function', columns='title_seniority', values='rate')

    # Reorder columns by seniority
    col_order = ['VP/Head', 'Director', 'Manager', 'IC']
    pivot_viz = pivot_viz[[c for c in col_order if c in pivot_viz.columns]]

    pivot_viz.plot(kind='bar', ax=ax1, colormap='viridis', edgecolor='white', width=0.8)

    ax1.set_title(f"The ICP Formula: Which Title Level Works Best in Which Dept for {product}?",
                  fontsize=14, fontweight='bold')
    ax1.set_ylabel("Conversion Rate", fontsize=12)
    ax1.set_xlabel("")
    ax1.set_ylim(0, 0.35)
    ax1.legend(title='Seniority Level', bbox_to_anchor=(1.02, 1), loc='upper left')
    ax1.axhline(subset['is_success'].mean(), color='red', linestyle='--', linewidth=2, label=f'{product} Avg')
    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')
    ax1.grid(axis='y', alpha=0.3)

    # RIGHT: Heatmap view
    ax2 = axes[1]
    pivot_heatmap = viz.pivot(index='title_function', columns='title_seniority', values='rate')
    pivot_heatmap = pivot_heatmap[[c for c in col_order if c in pivot_heatmap.columns]]

    sns.heatmap(pivot_heatmap, annot=True, fmt='.0%', cmap='RdYlGn',
                center=subset['is_success'].mean(), ax=ax2, linewidths=0.5)
    ax2.set_title(f'{product} Conversion Rate: Function √ó Seniority', fontsize=14, fontweight='bold')

    plt.tight_layout()
    plt.show()

if not df.empty:
    plot_power_trio(df, 'Mx')
```

## 5.4 Power Scope Analysis (Global vs Site)

Does having "Global" in your title make you a better Mx buyer?

```{python}
#| label: scope-analysis
#| fig-cap: "Power Scope: Do 'Global' titles convert better than 'Site' titles?"

def analyze_scope(df, product='Mx'):
    subset = df[df['product_segment'] == product].copy()

    if len(subset) < 50:
        return

    # Calculate stats by scope
    scope_stats = subset.groupby('title_scope').agg(
        n=('is_success', 'size'),
        successes=('is_success', 'sum')
    ).reset_index()

    # Add Bayesian estimates
    scope_stats['rate'] = scope_stats.apply(
        lambda r: bayesian_conversion_rate(r['successes'], r['n'])[0], axis=1)
    scope_stats['ci_low'] = scope_stats.apply(
        lambda r: bayesian_conversion_rate(r['successes'], r['n'])[1], axis=1)
    scope_stats['ci_high'] = scope_stats.apply(
        lambda r: bayesian_conversion_rate(r['successes'], r['n'])[2], axis=1)

    scope_stats = scope_stats.sort_values('rate', ascending=True)

    # Visualization
    fig, ax = plt.subplots(figsize=(12, 6))

    y_pos = range(len(scope_stats))
    colors = [PROJECT_COLS['Success'] if r > subset['is_success'].mean()
              else PROJECT_COLS['Failure'] for r in scope_stats['rate']]

    bars = ax.barh(y_pos, scope_stats['rate'], color=colors, edgecolor='white', height=0.6)

    # Error bars and labels
    for i, row in scope_stats.iterrows():
        idx = list(scope_stats.index).index(i)
        ax.plot([row['ci_low'], row['ci_high']], [idx, idx], color='black', linewidth=2)
        ax.text(row['rate'] + 0.01, idx, f"{row['rate']:.1%} (n={row['n']:,})",
               va='center', fontsize=11, fontweight='bold')

    ax.set_yticks(y_pos)
    ax.set_yticklabels(scope_stats['title_scope'], fontsize=12)
    ax.axvline(x=subset['is_success'].mean(), color='gray', linestyle='--',
               alpha=0.7, linewidth=2, label=f'{product} Average')
    ax.set_xlabel('Conversion Rate', fontsize=12)
    ax.set_title(f'Impact of Title Scope on {product} Conversion\n("Global VP" vs "Site Manager")',
                fontweight='bold', fontsize=14)
    ax.legend(loc='lower right')
    ax.set_xlim(0, 0.30)
    ax.grid(axis='x', alpha=0.3)

    plt.tight_layout()
    plt.show()

    # Calculate lift
    global_rate = scope_stats[scope_stats['title_scope'] == 'Global']['rate'].values
    standard_rate = scope_stats[scope_stats['title_scope'] == 'Standard']['rate'].values

    if len(global_rate) > 0 and len(standard_rate) > 0 and standard_rate[0] > 0:
        lift = global_rate[0] / standard_rate[0]
        print(f"\nüéØ KEY FINDING: 'Global' titles show {lift:.1f}x conversion lift vs Standard")

if not df.empty:
    analyze_scope(df, 'Mx')
```

## 5.5 Pipeline Velocity Analysis

Are Mx leads slower to convert? This tells us if the problem is targeting or sales enablement.

```{python}
#| label: velocity
#| fig-cap: "Pipeline Velocity: Are Mx leads stalling longer than Qx?"

def analyze_velocity(df):
    df_velocity = df[df['product_segment'].isin(['Mx', 'Qx'])].copy()
    df_success = df_velocity[df_velocity['is_success'] == 1].copy()

    if len(df_success) < 20:
        return

    # Stats by product
    velocity_stats = df_success.groupby('product_segment').agg(
        median_age=('lead_age_days', 'median'),
        mean_age=('lead_age_days', 'mean'),
        p25=('lead_age_days', lambda x: x.quantile(0.25)),
        p75=('lead_age_days', lambda x: x.quantile(0.75)),
        n=('is_success', 'sum')
    ).reset_index()

    print("=" * 60)
    print("PIPELINE VELOCITY ANALYSIS")
    print("=" * 60)
    print(velocity_stats.to_string(index=False))

    # Visualization
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # LEFT: Distribution
    ax1 = axes[0]
    for product, color in [('Mx', PROJECT_COLS['Mx']), ('Qx', PROJECT_COLS['Qx'])]:
        data = df_success[df_success['product_segment'] == product]['lead_age_days'].dropna()
        if len(data) > 5:
            ax1.hist(data, bins=25, alpha=0.6, label=f'{product} (n={len(data)})',
                    color=color, density=True, edgecolor='white')

    ax1.set_xlabel('Lead Age at Conversion (Days)', fontsize=11)
    ax1.set_ylabel('Density', fontsize=11)
    ax1.set_title('Time-to-Convert Distribution', fontweight='bold')
    ax1.legend()
    ax1.grid(alpha=0.3)

    # RIGHT: Box plot
    ax2 = axes[1]
    df_plot = df_success[['product_segment', 'lead_age_days']].dropna()

    bp = df_plot.boxplot(column='lead_age_days', by='product_segment', ax=ax2,
                         patch_artist=True, return_type='dict')

    colors = [PROJECT_COLS['Mx'], PROJECT_COLS['Qx']]
    for patch, color in zip(bp['lead_age_days']['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax2.set_xlabel('Product', fontsize=11)
    ax2.set_ylabel('Lead Age (Days)', fontsize=11)
    ax2.set_title('Time-to-Convert Comparison', fontweight='bold')
    plt.suptitle('')

    plt.tight_layout()
    plt.show()

    # Statistical test
    mx_ages = df_success[df_success['product_segment'] == 'Mx']['lead_age_days'].dropna()
    qx_ages = df_success[df_success['product_segment'] == 'Qx']['lead_age_days'].dropna()

    if len(mx_ages) > 5 and len(qx_ages) > 5:
        stat, p_val = mannwhitneyu(mx_ages, qx_ages, alternative='two-sided')
        mx_med = mx_ages.median()
        qx_med = qx_ages.median()
        diff = mx_med - qx_med

        print(f"\nüö® KEY FINDING: Mx deals take {abs(diff):.0f} days {'longer' if diff > 0 else 'shorter'} (median)")
        print(f"   Mann-Whitney p-value: {p_val:.4f}")

if not df.empty:
    analyze_velocity(df)
```

## 5.6 Driver Analysis (Entropy Scoring)

We use **Mutual Information (Information Theory)** to mathematically rank which features are the strongest predictors.

```{python}
#| label: feature-importance
#| fig-cap: "Driver Rank: Mathematical importance of each feature based on Mutual Information."

def rank_features(df, product='Mx'):
    if product:
        subset = df[df['product_segment'] == product].copy()
    else:
        subset = df.copy()

    # Select features
    features = ['acct_target_industry', 'acct_manufacturing_model',
                'title_seniority', 'title_function', 'title_scope',
                'is_decision_maker', 'completeness_tier',
                'priority', 'last_tactic_campaign_channel']

    features = [f for f in features if f in subset.columns]

    clean_df = subset.dropna(subset=features)
    if len(clean_df) < 50:
        return None

    X = clean_df[features].astype(str).apply(LabelEncoder().fit_transform)
    y = clean_df['is_success']

    # Calculate Mutual Information
    mi = mutual_info_classif(X, y, discrete_features=True, random_state=42)

    mi_df = pd.DataFrame({
        'Feature': features,
        'Importance': mi
    }).sort_values('Importance', ascending=False)

    print("=" * 60)
    print(f"FEATURE IMPORTANCE ({product if product else 'All'})")
    print("=" * 60)
    print(mi_df.to_string(index=False))

    # Visualization
    fig, ax = plt.subplots(figsize=(12, 7))

    colors = [PROJECT_COLS['Success'] if imp > mi_df['Importance'].median()
              else PROJECT_COLS['Neutral'] for imp in mi_df['Importance']]

    bars = ax.barh(range(len(mi_df)), mi_df['Importance'], color=colors, edgecolor='white')
    ax.set_yticks(range(len(mi_df)))
    ax.set_yticklabels(mi_df['Feature'], fontsize=11)
    ax.invert_yaxis()
    ax.set_xlabel("Mutual Information Score (bits)", fontsize=12)
    ax.set_title(f"Driver Analysis: Which attributes drive {product} conversion?",
                 fontsize=14, fontweight='bold')
    ax.grid(axis='x', alpha=0.3)

    # Add value labels
    for i, (imp, feat) in enumerate(zip(mi_df['Importance'], mi_df['Feature'])):
        ax.text(imp + 0.001, i, f'{imp:.4f}', va='center', fontsize=10)

    plt.tight_layout()
    plt.show()

    return mi_df

if not df.empty:
    mi_results = rank_features(df, 'Mx')
```

## 5.7 Decision Tree: Algorithmic Segment Discovery

Let the algorithm find the optimal targeting rules automatically.

```{python}
#| label: decision-tree
#| fig-cap: "Algorithmic targeting: The machine finds segments humans miss."

def discover_rules(df, product='Mx'):
    subset = df[df['product_segment'] == product].copy()

    features = ['title_seniority', 'title_function', 'acct_target_industry',
                'acct_manufacturing_model', 'title_scope']
    features = [f for f in features if f in subset.columns]

    # Encode
    X = subset[features].astype(str).copy()
    encoders = {}
    for col in features:
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col])
        encoders[col] = le

    y = subset['is_success']

    # Fit tree
    tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=30,
                                   min_samples_split=60, random_state=42)
    tree.fit(X, y)

    # Feature importance from tree
    importance = pd.DataFrame({
        'feature': features,
        'importance': tree.feature_importances_
    }).sort_values('importance', ascending=False)

    print("=" * 60)
    print(f"DECISION TREE FEATURE IMPORTANCE ({product})")
    print("=" * 60)
    print(importance.to_string(index=False))

    # Visualize tree
    fig, ax = plt.subplots(figsize=(20, 10))
    plot_tree(tree, feature_names=features, class_names=['Fail', 'Success'],
              filled=True, rounded=True, ax=ax, fontsize=10, max_depth=3)
    ax.set_title(f'{product} Targeting Decision Rules\n(Read: IF condition THEN outcome)',
                 fontweight='bold', fontsize=14)
    plt.tight_layout()
    plt.show()

    return tree, importance

if not df.empty:
    tree_model, tree_importance = discover_rules(df, 'Mx')
```

---

# The "Zombie" Lead Opportunity

"Recycled" leads are NOT failures - they're timing mismatches. This is a nurture campaign opportunity.

```{python}
#| label: zombie-analysis
#| fig-cap: "Zombie Leads: Where are we recycling high-potential prospects?"

def analyze_zombies(df):
    df_products = df[df['product_segment'].isin(['Mx', 'Qx'])].copy()

    mx_zombies = df_products[(df_products['product_segment'] == 'Mx') &
                              (df_products['outcome_tier'] == 'Near-Miss')]

    if len(mx_zombies) < 10:
        return

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # Top-left: Zombies by industry
    ax1 = axes[0, 0]
    zombie_industry = mx_zombies['acct_target_industry'].value_counts().head(8)
    zombie_industry.plot(kind='barh', ax=ax1, color=PROJECT_COLS['Neutral'])
    ax1.set_xlabel('Count of Recycled Leads')
    ax1.set_title('Mx "Zombie" Leads by Industry\n(Nurture Campaign Targets)', fontweight='bold')

    # Top-right: Zombies by seniority
    ax2 = axes[0, 1]
    zombie_seniority = mx_zombies['title_seniority'].value_counts()
    zombie_seniority.plot(kind='barh', ax=ax2, color=PROJECT_COLS['Highlight'])
    ax2.set_xlabel('Count of Recycled Leads')
    ax2.set_title('Mx "Zombie" Leads by Seniority', fontweight='bold')

    # Bottom-left: Zombies by function
    ax3 = axes[1, 0]
    zombie_function = mx_zombies['title_function'].value_counts()
    zombie_function.plot(kind='barh', ax=ax3, color=PROJECT_COLS['Success'])
    ax3.set_xlabel('Count of Recycled Leads')
    ax3.set_title('Mx "Zombie" Leads by Function', fontweight='bold')

    # Bottom-right: Decision maker split
    ax4 = axes[1, 1]
    dm_split = mx_zombies['is_decision_maker'].value_counts()
    dm_split.index = ['Non-Decision Maker', 'Decision Maker']
    colors = [PROJECT_COLS['Failure'], PROJECT_COLS['Success']]
    dm_split.plot(kind='pie', ax=ax4, colors=colors, autopct='%1.0f%%', startangle=90)
    ax4.set_ylabel('')
    ax4.set_title('Zombie Lead Quality\n(Decision Maker Status)', fontweight='bold')

    plt.tight_layout()
    plt.show()

    print(f"\nüßü ZOMBIE LEAD SUMMARY (Mx Recycled):")
    print(f"   Total Mx Zombies: {len(mx_zombies):,}")
    print(f"   Decision Maker Rate: {mx_zombies['is_decision_maker'].mean():.1%}")
    print(f"\n   ‚Üí These are high-value re-engagement targets!")

if not df.empty:
    analyze_zombies(df)
```

---

# Executive Summary Dashboard

```{python}
#| label: executive-dashboard
#| fig-cap: "Executive Dashboard: The complete picture at a glance."

def executive_dashboard(df):
    if df.empty:
        return

    df_mx = df[df['product_segment'] == 'Mx']
    df_qx = df[df['product_segment'] == 'Qx']

    fig = plt.figure(figsize=(18, 12))

    # Create grid
    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

    # 1. KPI Cards (top row)
    ax_kpi = fig.add_subplot(gs[0, :])
    ax_kpi.axis('off')

    mx_rate = df_mx['is_success'].mean()
    qx_rate = df_qx['is_success'].mean()
    gap = qx_rate - mx_rate
    zombies = len(df_mx[df_mx['outcome_tier'] == 'Near-Miss'])
    dm_rate = df_mx['is_decision_maker'].mean()

    kpi_text = f"""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë   MX CONVERSION   ‚ïë   QX CONVERSION   ‚ïë   PERFORMANCE GAP ‚ïë   ZOMBIE LEADS    ‚ïë
    ‚ïë      {mx_rate:.1%}         ‚ïë      {qx_rate:.1%}         ‚ïë      {gap:.1%}         ‚ïë      {zombies:,}          ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    ax_kpi.text(0.5, 0.5, kpi_text, fontsize=14, fontfamily='monospace',
                ha='center', va='center', transform=ax_kpi.transAxes)
    ax_kpi.set_title('KEY PERFORMANCE INDICATORS', fontsize=16, fontweight='bold')

    # 2. Conversion by Seniority
    ax2 = fig.add_subplot(gs[1, 0])
    seniority_rates = df_mx.groupby('title_seniority')['is_success'].mean().sort_values()
    colors = [PROJECT_COLS['Success'] if r > mx_rate else PROJECT_COLS['Failure'] for r in seniority_rates]
    seniority_rates.plot(kind='barh', ax=ax2, color=colors)
    ax2.axvline(mx_rate, color='red', linestyle='--', linewidth=2)
    ax2.set_title('Mx by Seniority', fontweight='bold')
    ax2.set_xlabel('Conversion Rate')

    # 3. Conversion by Function
    ax3 = fig.add_subplot(gs[1, 1])
    function_rates = df_mx.groupby('title_function')['is_success'].mean().sort_values()
    colors = [PROJECT_COLS['Success'] if r > mx_rate else PROJECT_COLS['Failure'] for r in function_rates]
    function_rates.plot(kind='barh', ax=ax3, color=colors)
    ax3.axvline(mx_rate, color='red', linestyle='--', linewidth=2)
    ax3.set_title('Mx by Function', fontweight='bold')
    ax3.set_xlabel('Conversion Rate')

    # 4. Conversion by Scope
    ax4 = fig.add_subplot(gs[1, 2])
    scope_rates = df_mx.groupby('title_scope')['is_success'].mean().sort_values()
    colors = [PROJECT_COLS['Success'] if r > mx_rate else PROJECT_COLS['Failure'] for r in scope_rates]
    scope_rates.plot(kind='barh', ax=ax4, color=colors)
    ax4.axvline(mx_rate, color='red', linestyle='--', linewidth=2)
    ax4.set_title('Mx by Scope', fontweight='bold')
    ax4.set_xlabel('Conversion Rate')

    # 5. Outcome Distribution
    ax5 = fig.add_subplot(gs[2, 0])
    outcome_counts = df_mx['outcome_tier'].value_counts()
    colors = [PROJECT_COLS['Failure'], PROJECT_COLS['Neutral'], PROJECT_COLS['Success']]
    outcome_counts.plot(kind='pie', ax=ax5, colors=colors, autopct='%1.0f%%')
    ax5.set_ylabel('')
    ax5.set_title('Mx Outcome Distribution', fontweight='bold')

    # 6. Top Industries
    ax6 = fig.add_subplot(gs[2, 1])
    industry_rates = df_mx.groupby('acct_target_industry').agg(
        rate=('is_success', 'mean'),
        n=('is_success', 'size')
    )
    industry_rates = industry_rates[industry_rates['n'] >= 30].sort_values('rate', ascending=True).tail(8)
    industry_rates['rate'].plot(kind='barh', ax=ax6, color=PROJECT_COLS['Success'])
    ax6.set_title('Top Industries for Mx', fontweight='bold')
    ax6.set_xlabel('Conversion Rate')

    # 7. Recommendations
    ax7 = fig.add_subplot(gs[2, 2])
    ax7.axis('off')

    reco_text = """
    RECOMMENDED ACTIONS:

    ‚úì Target: Directors/VPs with
       Global scope in Pharma

    ‚úì Avoid: ICs and Site-level
       roles in Unknown industries

    ‚úì Speed: Implement 24-hour
       SLA for Mx leads

    ‚úì Nurture: Re-engage the
       {zombies:,} recycled leads
    """.format(zombies=zombies)

    ax7.text(0.1, 0.5, reco_text, fontsize=12, fontfamily='monospace',
             va='center', transform=ax7.transAxes,
             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
    ax7.set_title('ACTION ITEMS', fontweight='bold')

    plt.suptitle('MasterControl Mx Performance Dashboard', fontsize=20, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.show()

if not df.empty:
    executive_dashboard(df)
```

---

# Deployment

We export the "Enriched" dataset with our engineered features.

```{python}
#| label: export
#| eval: false

def export_enriched_dataset(df):
    export_cols = [
        # IDs
        'qal_id', 'contact_lead_id',

        # Targets
        'is_success', 'outcome_tier', 'next_stage__c', 'product_segment',

        # Account Features
        'acct_target_industry', 'acct_manufacturing_model',
        'acct_primary_site_function', 'acct_territory_rollup',

        # Engineered Features
        'title_seniority', 'title_function', 'title_scope', 'is_decision_maker',
        'record_completeness', 'completeness_tier',

        # Lead attributes
        'priority', 'last_tactic_campaign_channel',

        # Temporal
        'cohort_date', 'cohort_year', 'cohort_quarter', 'lead_age_days'
    ]

    export_cols = [c for c in export_cols if c in df.columns]
    df_export = df[export_cols].copy()

    output_path = Path.cwd() / "Cleaned_QAL_Competition_Grade.csv"
    df_export.to_csv(output_path, index=False)

    print(f"‚úì Exported: {output_path}")
    print(f"  Rows: {len(df_export):,}")
    print(f"  Columns: {len(df_export.columns)}")

    return df_export

if not df.empty:
    df_export = export_enriched_dataset(df)
```

---

# Appendix: Statistical Methodology

**Bayesian Credible Intervals:** We use Beta-Binomial conjugacy with uninformative priors (alpha=1, beta=1) to estimate conversion rates. This provides more robust uncertainty quantification for small samples compared to frequentist confidence intervals.

**Mutual Information:** Feature importance is calculated using Shannon mutual information, which measures the reduction in uncertainty about the target variable given knowledge of each feature.

**Decision Tree Interactions:** We use shallow decision trees (depth <= 3) as interaction detectors. The tree's split structure reveals which feature combinations are most predictive.

**Log-Odds Ratio:** For semantic analysis, we calculate log-odds with Laplace smoothing to identify words that discriminate between successful and unsuccessful leads.

---

*Document generated for MSBA Capstone Case Competition - Spring 2026*
