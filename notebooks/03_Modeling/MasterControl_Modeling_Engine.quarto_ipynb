{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"The Revenue Engine V2: MasterControl Advanced Modeling\"\n",
        "subtitle: \"Maximum Predictive Power for Mx Lead Prioritization\"\n",
        "author: \"MSBA Capstone Group 3\"\n",
        "date: \"Spring 2026\"\n",
        "format:\n",
        "  html:\n",
        "    theme: journal\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    df-print: paged\n",
        "    code-fold: true\n",
        "    code-tools: true\n",
        "  pdf:\n",
        "    documentclass: article\n",
        "    geometry:\n",
        "      - top=1in\n",
        "      - bottom=1in\n",
        "      - left=0.75in\n",
        "      - right=0.75in\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "    colorlinks: true\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: false\n",
        "  message: false\n",
        "editor: visual\n",
        "---\n",
        "\n",
        "# Executive Summary\n",
        "\n",
        "**The Mission:** Push our baseline model (AUC ~0.86) to maximum predictive power using advanced feature engineering, hyperparameter-tuned ensemble models, and production-grade interpretability.\n",
        "\n",
        "**V2 Upgrades:**\n",
        "\n",
        "1.  **Latent Semantic Analysis (LSA):** Dense semantic embeddings for job titles\n",
        "2.  **Polynomial Interaction Features:** Explicit \"VP √ó Operations\" cross-products\n",
        "3.  **Target Encoding:** Smooth win-rate encoding for high-cardinality industries\n",
        "4.  **Hyperparameter Tuning:** GridSearchCV for XGBoost & LightGBM\n",
        "5.  **Voting Ensemble:** Soft-voting combination of top performers\n",
        "6.  **Revenue Curve:** Dollar-denominated business impact visualization\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 1: Production Environment Setup"
      ],
      "id": "06ab3f83"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: setup\n",
        "\n",
        "# ==============================================================================\n",
        "# PRODUCTION ENVIRONMENT V2\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Scikit-learn Core\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, cross_val_score, StratifiedKFold,\n",
        "    GridSearchCV, RandomizedSearchCV, cross_val_predict\n",
        ")\n",
        "from sklearn.preprocessing import (\n",
        "    StandardScaler, OneHotEncoder, LabelEncoder,\n",
        "    FunctionTransformer, PolynomialFeatures\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, VotingClassifier,\n",
        "    GradientBoostingClassifier\n",
        ")\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# XGBoost & LightGBM\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, precision_recall_curve, auc,\n",
        "    log_loss, classification_report, confusion_matrix,\n",
        "    roc_curve, precision_score, recall_score, f1_score,\n",
        "    make_scorer\n",
        ")\n",
        "\n",
        "# Calibration\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Interpretability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è SHAP not available. Install with: pip install shap\")\n",
        "\n",
        "# Parallelization\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "\n",
        "# ==============================================================================\n",
        "# GLOBAL CONFIGURATION\n",
        "# ==============================================================================\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "N_JOBS = -1  # Use all cores\n",
        "CV_FOLDS = 5\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "# Business Parameters\n",
        "AVG_DEAL_SIZE = 50000  # $50k average deal\n",
        "CONVERSION_TO_DEAL = 0.12  # 12% of SQLs become deals\n",
        "\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Project Colors (The Golden Palette)\n",
        "PROJECT_COLS = {\n",
        "    'Success': '#00534B',   # MasterControl Teal\n",
        "    'Failure': '#F05627',   # Risk Orange\n",
        "    'Neutral': '#95a5a6',   # Gray\n",
        "    'Highlight': '#2980b9', # Blue\n",
        "    'Gold': '#f39c12',      # Accent Gold\n",
        "    'Purple': '#9b59b6'     # Accent Purple\n",
        "}\n",
        "\n",
        "# Plotting configuration\n",
        "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['axes.titleweight'] = 'bold'\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ PRODUCTION MODELING ENVIRONMENT V2\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úì Random State: {RANDOM_STATE}\")\n",
        "print(f\"‚úì CPU Cores: {multiprocessing.cpu_count()}\")\n",
        "print(f\"‚úì CV Folds: {CV_FOLDS}\")\n",
        "print(f\"‚úì XGBoost: Available\")\n",
        "print(f\"‚úì LightGBM: Available\")\n",
        "print(f\"‚úì SHAP: {SHAP_AVAILABLE}\")\n",
        "print(f\"‚úì Avg Deal Size: ${AVG_DEAL_SIZE:,}\")\n",
        "print(\"=\" * 70)"
      ],
      "id": "setup",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 2: Data Loading & Advanced Feature Engineering"
      ],
      "id": "599a2308"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: data-loading\n",
        "\n",
        "# ==============================================================================\n",
        "# DATA LOADING\n",
        "# ==============================================================================\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load raw data with intelligent path detection.\"\"\"\n",
        "    possible_paths = [\n",
        "        Path.cwd() / \"data\" / \"QAL Performance for MSBA.csv\",\n",
        "        Path.cwd().parent / \"data\" / \"QAL Performance for MSBA.csv\",\n",
        "        Path.cwd().parent.parent / \"data\" / \"QAL Performance for MSBA.csv\",\n",
        "        Path.cwd().parent.parent.parent / \"data\" / \"QAL Performance for MSBA.csv\"\n",
        "    ]\n",
        "\n",
        "    for p in possible_paths:\n",
        "        if p.exists():\n",
        "            df = pd.read_csv(p)\n",
        "            print(f\"‚úì Data loaded from: {p}\")\n",
        "            return df\n",
        "\n",
        "    raise FileNotFoundError(\"Could not find QAL Performance for MSBA.csv\")\n",
        "\n",
        "df_raw = load_data()\n",
        "\n",
        "# Standardize column names\n",
        "df_raw.columns = [c.strip().lower().replace(' ', '_').replace('/', '_').replace('-', '_')\n",
        "                  for c in df_raw.columns]\n",
        "\n",
        "print(f\"‚úì Raw Data Shape: {df_raw.shape}\")"
      ],
      "id": "data-loading",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: feature-engineering\n",
        "\n",
        "# ==============================================================================\n",
        "# ADVANCED FEATURE ENGINEERING PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "def engineer_features(df):\n",
        "    \"\"\"\n",
        "    V2 Feature Engineering with Advanced Techniques.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 1. TARGET VARIABLE\n",
        "    # -------------------------------------------------------------------------\n",
        "    success_stages = ['SQL', 'SQO', 'Won']\n",
        "    df['is_success'] = df['next_stage__c'].isin(success_stages).astype(int)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 2. PRODUCT SEGMENTATION\n",
        "    # -------------------------------------------------------------------------\n",
        "    def segment_product(sol):\n",
        "        if str(sol) == 'Mx': return 'Mx'\n",
        "        elif str(sol) == 'Qx': return 'Qx'\n",
        "        return 'Other'\n",
        "    df['product_segment'] = df['solution_rollup'].apply(segment_product)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 3. TITLE PARSING (Enhanced)\n",
        "    # -------------------------------------------------------------------------\n",
        "    def parse_seniority(t):\n",
        "        if pd.isna(t): return 'Unknown'\n",
        "        t = str(t).lower()\n",
        "        if re.search(r'\\b(ceo|cfo|coo|cto|cio|chief|c-level|president|founder|owner)\\b', t):\n",
        "            return 'C-Suite'\n",
        "        if re.search(r'\\b(svp|senior vice president|evp)\\b', t):\n",
        "            return 'SVP'\n",
        "        if re.search(r'\\b(vp|vice president|head of)\\b', t):\n",
        "            return 'VP'\n",
        "        if re.search(r'\\b(director)\\b', t):\n",
        "            return 'Director'\n",
        "        if re.search(r'\\b(manager|mgr|lead|supervisor)\\b', t):\n",
        "            return 'Manager'\n",
        "        if re.search(r'\\b(analyst|engineer|specialist|associate|coordinator)\\b', t):\n",
        "            return 'IC'\n",
        "        return 'Other'\n",
        "\n",
        "    def parse_function(t):\n",
        "        if pd.isna(t): return 'Unknown'\n",
        "        t = str(t).lower()\n",
        "        if re.search(r'\\b(manuf|prod|ops|plant|supply|site|factory)\\b', t):\n",
        "            return 'Manufacturing_Ops'\n",
        "        if re.search(r'\\b(quality|qa|qc|qms|compliance|validation|capa)\\b', t):\n",
        "            return 'Quality_Reg'\n",
        "        if re.search(r'\\b(regulatory|reg affairs|submissions)\\b', t):\n",
        "            return 'Regulatory'\n",
        "        if re.search(r'\\b(it|info|sys|tech|data|soft)\\b', t):\n",
        "            return 'IT_Systems'\n",
        "        if re.search(r'\\b(lab|r&d|sci|dev|clin|research)\\b', t):\n",
        "            return 'R_D_Lab'\n",
        "        return 'Other'\n",
        "\n",
        "    def parse_scope(t):\n",
        "        if pd.isna(t): return 'Standard'\n",
        "        t = str(t).lower()\n",
        "        if re.search(r'\\b(global|worldwide|international|corporate|enterprise|group)\\b', t):\n",
        "            return 'Global'\n",
        "        if re.search(r'\\b(regional|division)\\b', t):\n",
        "            return 'Regional'\n",
        "        if re.search(r'\\b(site|plant|facility|local)\\b', t):\n",
        "            return 'Site'\n",
        "        return 'Standard'\n",
        "\n",
        "    df['title_seniority'] = df['contact_lead_title'].apply(parse_seniority)\n",
        "    df['title_function'] = df['contact_lead_title'].apply(parse_function)\n",
        "    df['title_scope'] = df['contact_lead_title'].apply(parse_scope)\n",
        "    df['is_decision_maker'] = df['title_seniority'].isin(\n",
        "        ['C-Suite', 'SVP', 'VP', 'Director']\n",
        "    ).astype(int)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 4. RECORD COMPLETENESS\n",
        "    # -------------------------------------------------------------------------\n",
        "    completeness_cols = [\n",
        "        'acct_manufacturing_model', 'acct_primary_site_function',\n",
        "        'acct_target_industry', 'acct_territory_rollup', 'acct_tier_rollup'\n",
        "    ]\n",
        "\n",
        "    def calc_completeness(row):\n",
        "        filled = sum(1 for col in completeness_cols\n",
        "                     if col in row.index and pd.notna(row[col])\n",
        "                     and str(row[col]).lower() not in ['unknown', 'nan', ''])\n",
        "        return filled / len(completeness_cols)\n",
        "\n",
        "    df['record_completeness'] = df.apply(calc_completeness, axis=1)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 5. TEMPORAL FEATURES\n",
        "    # -------------------------------------------------------------------------\n",
        "    df['cohort_date'] = pd.to_datetime(df['qal_cohort_date'], errors='coerce')\n",
        "    df['cohort_quarter'] = df['cohort_date'].dt.quarter.fillna(0).astype(int)\n",
        "    df['cohort_month'] = df['cohort_date'].dt.month.fillna(0).astype(int)\n",
        "    df['cohort_dayofweek'] = df['cohort_date'].dt.dayofweek.fillna(0).astype(int)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 6. IMPUTATION\n",
        "    # -------------------------------------------------------------------------\n",
        "    fill_cols = ['acct_target_industry', 'acct_manufacturing_model',\n",
        "                 'acct_territory_rollup', 'acct_primary_site_function']\n",
        "    for col in fill_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    df['contact_lead_title'] = df['contact_lead_title'].fillna('Unknown Title')\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # 7. HIGH-VALUE INTERACTION: Seniority x Function (Explicit)\n",
        "    # -------------------------------------------------------------------------\n",
        "    df['seniority_function'] = df['title_seniority'] + '_X_' + df['title_function']\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering\n",
        "df = engineer_features(df_raw)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚úÖ FEATURE ENGINEERING COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úì Total Records: {len(df):,}\")\n",
        "print(f\"‚úì Target Rate: {df['is_success'].mean():.1%}\")\n",
        "print(f\"‚úì Mx Leads: {len(df[df['product_segment']=='Mx']):,}\")\n",
        "print(f\"‚úì Unique Industries: {df['acct_target_industry'].nunique()}\")\n",
        "print(f\"‚úì Unique Seniority√óFunction: {df['seniority_function'].nunique()}\")"
      ],
      "id": "feature-engineering",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 3: Target Encoding & LSA Pipeline"
      ],
      "id": "f5100f6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: target-encoder\n",
        "\n",
        "# ==============================================================================\n",
        "# CUSTOM TARGET ENCODER (Smoothed)\n",
        "# ==============================================================================\n",
        "\n",
        "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Smoothed Target Encoding for high-cardinality categorical features.\n",
        "    Uses leave-one-out encoding to prevent target leakage.\n",
        "    \n",
        "    Formula: encoded = (count * mean + global_mean * smoothing) / (count + smoothing)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, smoothing=10, min_samples=5):\n",
        "        self.smoothing = smoothing\n",
        "        self.min_samples = min_samples\n",
        "        self.encoding_map_ = {}\n",
        "        self.global_mean_ = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X).ravel()\n",
        "        y = np.array(y).ravel()\n",
        "        \n",
        "        self.global_mean_ = y.mean()\n",
        "        \n",
        "        df = pd.DataFrame({'feature': X, 'target': y})\n",
        "        agg = df.groupby('feature')['target'].agg(['mean', 'count'])\n",
        "        \n",
        "        # Smoothed encoding\n",
        "        smoothed_mean = (\n",
        "            (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) /\n",
        "            (agg['count'] + self.smoothing)\n",
        "        )\n",
        "        \n",
        "        # Replace low-count categories with global mean\n",
        "        smoothed_mean[agg['count'] < self.min_samples] = self.global_mean_\n",
        "        \n",
        "        self.encoding_map_ = smoothed_mean.to_dict()\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X = np.array(X).ravel()\n",
        "        encoded = np.array([\n",
        "            self.encoding_map_.get(val, self.global_mean_) \n",
        "            for val in X\n",
        "        ]).reshape(-1, 1)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# LSA TEXT TRANSFORMER\n",
        "# ==============================================================================\n",
        "\n",
        "class LSATextTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Latent Semantic Analysis for text features.\n",
        "    TF-IDF ‚Üí TruncatedSVD ‚Üí Dense semantic components.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_components=15, max_features=500):\n",
        "        self.n_components = n_components\n",
        "        self.max_features = max_features\n",
        "        self.tfidf = None\n",
        "        self.svd = None\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        X = np.array(X).ravel()\n",
        "        X = [str(x).lower() if pd.notna(x) else 'unknown' for x in X]\n",
        "        \n",
        "        self.tfidf = TfidfVectorizer(\n",
        "            max_features=self.max_features,\n",
        "            stop_words='english',\n",
        "            ngram_range=(1, 2),\n",
        "            min_df=5,\n",
        "            max_df=0.95\n",
        "        )\n",
        "        \n",
        "        tfidf_matrix = self.tfidf.fit_transform(X)\n",
        "        \n",
        "        # LSA via TruncatedSVD\n",
        "        n_comp = min(self.n_components, tfidf_matrix.shape[1] - 1)\n",
        "        self.svd = TruncatedSVD(n_components=n_comp, random_state=RANDOM_STATE)\n",
        "        self.svd.fit(tfidf_matrix)\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        X = np.array(X).ravel()\n",
        "        X = [str(x).lower() if pd.notna(x) else 'unknown' for x in X]\n",
        "        \n",
        "        tfidf_matrix = self.tfidf.transform(X)\n",
        "        lsa_matrix = self.svd.transform(tfidf_matrix)\n",
        "        \n",
        "        return lsa_matrix\n",
        "    \n",
        "    def get_feature_names_out(self):\n",
        "        return [f'LSA_{i}' for i in range(self.svd.n_components)]\n",
        "\n",
        "\n",
        "print(\"‚úì Custom Transformers Defined: TargetEncoder, LSATextTransformer\")"
      ],
      "id": "target-encoder",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 4: Model-Ready Dataset"
      ],
      "id": "eac97a79"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: model-prep\n",
        "\n",
        "# ==============================================================================\n",
        "# PREPARE MX-FOCUSED DATASET\n",
        "# ==============================================================================\n",
        "\n",
        "# Filter to Mx leads only\n",
        "df_mx = df[df['product_segment'] == 'Mx'].copy()\n",
        "\n",
        "print(f\"‚úì Mx Dataset: {len(df_mx):,} leads\")\n",
        "print(f\"‚úì Mx Conversion Rate: {df_mx['is_success'].mean():.1%}\")\n",
        "\n",
        "# Define feature groups\n",
        "CATEGORICAL_LOW_CARD = [\n",
        "    'title_seniority',\n",
        "    'title_function',\n",
        "    'title_scope',\n",
        "    'acct_manufacturing_model',\n",
        "    'acct_territory_rollup'\n",
        "]\n",
        "\n",
        "CATEGORICAL_HIGH_CARD = [\n",
        "    'acct_target_industry'  # Target encode this\n",
        "]\n",
        "\n",
        "INTERACTION_FEATURES = [\n",
        "    'seniority_function'  # Pre-computed interaction\n",
        "]\n",
        "\n",
        "NUMERIC_FEATURES = [\n",
        "    'is_decision_maker',\n",
        "    'record_completeness',\n",
        "    'cohort_quarter',\n",
        "    'cohort_month',\n",
        "    'cohort_dayofweek'\n",
        "]\n",
        "\n",
        "TEXT_FEATURE = 'contact_lead_title'\n",
        "TARGET = 'is_success'\n",
        "\n",
        "# Filter to existing columns\n",
        "CATEGORICAL_LOW_CARD = [f for f in CATEGORICAL_LOW_CARD if f in df_mx.columns]\n",
        "CATEGORICAL_HIGH_CARD = [f for f in CATEGORICAL_HIGH_CARD if f in df_mx.columns]\n",
        "INTERACTION_FEATURES = [f for f in INTERACTION_FEATURES if f in df_mx.columns]\n",
        "NUMERIC_FEATURES = [f for f in NUMERIC_FEATURES if f in df_mx.columns]\n",
        "\n",
        "ALL_FEATURES = CATEGORICAL_LOW_CARD + CATEGORICAL_HIGH_CARD + INTERACTION_FEATURES + NUMERIC_FEATURES + [TEXT_FEATURE]\n",
        "\n",
        "print(f\"\\n‚úì Low-Card Categorical: {CATEGORICAL_LOW_CARD}\")\n",
        "print(f\"‚úì High-Card (Target Encode): {CATEGORICAL_HIGH_CARD}\")\n",
        "print(f\"‚úì Interaction Features: {INTERACTION_FEATURES}\")\n",
        "print(f\"‚úì Numeric: {NUMERIC_FEATURES}\")\n",
        "print(f\"‚úì Text (LSA): {TEXT_FEATURE}\")"
      ],
      "id": "model-prep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: train-test-split\n",
        "\n",
        "# ==============================================================================\n",
        "# STRATIFIED TRAIN/TEST SPLIT\n",
        "# ==============================================================================\n",
        "\n",
        "X = df_mx[ALL_FEATURES].copy()\n",
        "y = df_mx[TARGET].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"TRAIN/TEST SPLIT\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"‚úì Training: {len(X_train):,} ({len(X_train)/len(X):.0%})\")\n",
        "print(f\"‚úì Test: {len(X_test):,} ({len(X_test)/len(X):.0%})\")\n",
        "print(f\"‚úì Train Target Rate: {y_train.mean():.1%}\")\n",
        "print(f\"‚úì Test Target Rate: {y_test.mean():.1%}\")"
      ],
      "id": "train-test-split",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 5: Advanced Preprocessing Pipeline"
      ],
      "id": "28501e68"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: preprocessing\n",
        "\n",
        "# ==============================================================================\n",
        "# ADVANCED PREPROCESSING PIPELINE\n",
        "# ==============================================================================\n",
        "\n",
        "# 1. Numeric Pipeline\n",
        "numeric_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# 2. Low-Cardinality Categorical Pipeline (OneHot)\n",
        "categorical_low_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# 3. High-Cardinality Pipeline (Target Encoding)\n",
        "# We'll apply target encoding separately to avoid leakage\n",
        "\n",
        "# 4. Interaction Features Pipeline (OneHot with limit)\n",
        "interaction_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, \n",
        "                             max_categories=30))\n",
        "])\n",
        "\n",
        "# 5. LSA Text Pipeline\n",
        "lsa_pipeline = LSATextTransformer(n_components=15, max_features=500)\n",
        "\n",
        "# Build ColumnTransformer (without target encoding for now)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_pipeline, NUMERIC_FEATURES),\n",
        "        ('cat_low', categorical_low_pipeline, CATEGORICAL_LOW_CARD),\n",
        "        ('interaction', interaction_pipeline, INTERACTION_FEATURES),\n",
        "        ('lsa', lsa_pipeline, TEXT_FEATURE)\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    n_jobs=N_JOBS\n",
        ")\n",
        "\n",
        "# Fit preprocessor\n",
        "X_train_base = preprocessor.fit_transform(X_train)\n",
        "X_test_base = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"‚úì Base Features Shape: {X_train_base.shape}\")\n",
        "\n",
        "# Add Target Encoding for high-cardinality features\n",
        "if CATEGORICAL_HIGH_CARD:\n",
        "    target_encoders = {}\n",
        "    target_encoded_train = []\n",
        "    target_encoded_test = []\n",
        "    \n",
        "    for col in CATEGORICAL_HIGH_CARD:\n",
        "        te = TargetEncoder(smoothing=10, min_samples=5)\n",
        "        te.fit(X_train[col], y_train)\n",
        "        target_encoders[col] = te\n",
        "        \n",
        "        target_encoded_train.append(te.transform(X_train[col]))\n",
        "        target_encoded_test.append(te.transform(X_test[col]))\n",
        "    \n",
        "    target_train = np.hstack(target_encoded_train)\n",
        "    target_test = np.hstack(target_encoded_test)\n",
        "    \n",
        "    X_train_processed = np.hstack([X_train_base, target_train])\n",
        "    X_test_processed = np.hstack([X_test_base, target_test])\n",
        "    \n",
        "    print(f\"‚úì + Target Encoded: {target_train.shape[1]} features\")\n",
        "else:\n",
        "    X_train_processed = X_train_base\n",
        "    X_test_processed = X_test_base\n",
        "\n",
        "print(f\"‚úì Final Features Shape: {X_train_processed.shape}\")\n",
        "\n",
        "# Get feature names\n",
        "def get_feature_names():\n",
        "    names = []\n",
        "    \n",
        "    # Numeric\n",
        "    names.extend([f'num_{c}' for c in NUMERIC_FEATURES])\n",
        "    \n",
        "    # Low-card categorical\n",
        "    try:\n",
        "        ohe = preprocessor.named_transformers_['cat_low'].named_steps['onehot']\n",
        "        names.extend(ohe.get_feature_names_out(CATEGORICAL_LOW_CARD))\n",
        "    except:\n",
        "        names.extend([f'cat_{c}' for c in CATEGORICAL_LOW_CARD])\n",
        "    \n",
        "    # Interaction\n",
        "    try:\n",
        "        ohe = preprocessor.named_transformers_['interaction'].named_steps['onehot']\n",
        "        names.extend(ohe.get_feature_names_out(INTERACTION_FEATURES))\n",
        "    except:\n",
        "        names.extend([f'int_{c}' for c in INTERACTION_FEATURES])\n",
        "    \n",
        "    # LSA\n",
        "    names.extend([f'LSA_{i}' for i in range(15)])\n",
        "    \n",
        "    # Target encoded\n",
        "    names.extend([f'TE_{c}' for c in CATEGORICAL_HIGH_CARD])\n",
        "    \n",
        "    return names\n",
        "\n",
        "FEATURE_NAMES = get_feature_names()\n",
        "print(f\"‚úì Total Feature Names: {len(FEATURE_NAMES)}\")"
      ],
      "id": "preprocessing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 6: The Super-Model Tournament"
      ],
      "id": "3eaeee9a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: model-definitions\n",
        "\n",
        "# ==============================================================================\n",
        "# MODEL DEFINITIONS WITH HYPERPARAMETER GRIDS\n",
        "# ==============================================================================\n",
        "\n",
        "# Calculate class weight for imbalanced data\n",
        "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
        "print(f\"‚úì Class Imbalance Ratio: {pos_weight:.2f}:1\")\n",
        "\n",
        "# Base models\n",
        "models = {}\n",
        "\n",
        "# 1. LOGISTIC REGRESSION (LASSO)\n",
        "models['Logistic_LASSO'] = {\n",
        "    'model': LogisticRegression(\n",
        "        penalty='l1',\n",
        "        solver='saga',\n",
        "        max_iter=1000,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=N_JOBS,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'params': {\n",
        "        'C': [0.01, 0.1, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. RANDOM FOREST\n",
        "models['Random_Forest'] = {\n",
        "    'model': RandomForestClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=N_JOBS,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'params': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [8, 12],\n",
        "        'min_samples_leaf': [10, 20]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 3. XGBOOST (Tuned)\n",
        "models['XGBoost'] = {\n",
        "    'model': xgb.XGBClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=N_JOBS,\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss'\n",
        "    ),\n",
        "    'params': {\n",
        "        'n_estimators': [150, 250],\n",
        "        'max_depth': [4, 6, 8],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'scale_pos_weight': [1, pos_weight],\n",
        "        'subsample': [0.8],\n",
        "        'colsample_bytree': [0.8]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 4. LIGHTGBM (Fast & Powerful)\n",
        "models['LightGBM'] = {\n",
        "    'model': lgb.LGBMClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=N_JOBS,\n",
        "        verbose=-1\n",
        "    ),\n",
        "    'params': {\n",
        "        'n_estimators': [150, 250],\n",
        "        'max_depth': [4, 6, 8],\n",
        "        'learning_rate': [0.05, 0.1],\n",
        "        'scale_pos_weight': [1, pos_weight],\n",
        "        'num_leaves': [31, 63]\n",
        "    }\n",
        "}\n",
        "\n",
        "# 5. NEURAL NETWORK (MLP with Dropout-like regularization)\n",
        "models['Neural_Network'] = {\n",
        "    'model': MLPClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        n_iter_no_change=15\n",
        "    ),\n",
        "    'params': {\n",
        "        'hidden_layer_sizes': [(128, 64), (256, 128, 64)],\n",
        "        'alpha': [0.001, 0.01],  # L2 regularization (dropout proxy)\n",
        "        'learning_rate_init': [0.001, 0.01],\n",
        "        'batch_size': [64, 128]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üèÜ SUPER-MODEL TOURNAMENT\")\n",
        "print(\"=\" * 70)\n",
        "for name in models:\n",
        "    print(f\"  ‚Ä¢ {name}\")"
      ],
      "id": "model-definitions",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: model-training\n",
        "\n",
        "# ==============================================================================\n",
        "# HYPERPARAMETER TUNING & TRAINING\n",
        "# ==============================================================================\n",
        "\n",
        "cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "results = {}\n",
        "best_estimators = {}\n",
        "\n",
        "print(f\"\\nüîÑ Training with {CV_FOLDS}-fold CV + GridSearchCV...\\n\")\n",
        "\n",
        "for name, config in models.items():\n",
        "    print(f\"‚è≥ {name}...\", end=\" \")\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    # GridSearchCV\n",
        "    grid = GridSearchCV(\n",
        "        config['model'],\n",
        "        config['params'],\n",
        "        cv=cv,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=N_JOBS,\n",
        "        refit=True\n",
        "    )\n",
        "    \n",
        "    grid.fit(X_train_processed, y_train)\n",
        "    \n",
        "    best_model = grid.best_estimator_\n",
        "    best_estimators[name] = best_model\n",
        "    \n",
        "    # Test predictions\n",
        "    test_probs = best_model.predict_proba(X_test_processed)[:, 1]\n",
        "    test_preds = best_model.predict(X_test_processed)\n",
        "    \n",
        "    # Metrics\n",
        "    test_auc = roc_auc_score(y_test, test_probs)\n",
        "    test_logloss = log_loss(y_test, test_probs)\n",
        "    precision, recall, _ = precision_recall_curve(y_test, test_probs)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    \n",
        "    elapsed = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': best_model,\n",
        "        'best_params': grid.best_params_,\n",
        "        'cv_auc': grid.best_score_,\n",
        "        'test_auc': test_auc,\n",
        "        'test_logloss': test_logloss,\n",
        "        'pr_auc': pr_auc,\n",
        "        'test_probs': test_probs,\n",
        "        'test_preds': test_preds,\n",
        "        'train_time': elapsed\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úì AUC={test_auc:.4f} (Time: {elapsed:.1f}s)\")\n",
        "\n",
        "print(\"\\n‚úÖ All base models trained!\")"
      ],
      "id": "model-training",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: voting-ensemble\n",
        "\n",
        "# ==============================================================================\n",
        "# VOTING ENSEMBLE (THE CLOSER)\n",
        "# ==============================================================================\n",
        "\n",
        "# Select top 3 models by test AUC\n",
        "sorted_models = sorted(results.items(), key=lambda x: x[1]['test_auc'], reverse=True)\n",
        "top_3 = sorted_models[:3]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ VOTING ENSEMBLE: Combining Top 3 Models\")\n",
        "print(\"=\" * 70)\n",
        "for name, r in top_3:\n",
        "    print(f\"  ‚Ä¢ {name}: AUC={r['test_auc']:.4f}\")\n",
        "\n",
        "# Create voting classifier\n",
        "voting_estimators = [(name, best_estimators[name]) for name, _ in top_3]\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=voting_estimators,\n",
        "    voting='soft',\n",
        "    n_jobs=N_JOBS\n",
        ")\n",
        "\n",
        "print(\"\\n‚è≥ Training Voting Ensemble...\")\n",
        "start_time = datetime.now()\n",
        "ensemble.fit(X_train_processed, y_train)\n",
        "\n",
        "# Ensemble predictions\n",
        "ensemble_probs = ensemble.predict_proba(X_test_processed)[:, 1]\n",
        "ensemble_preds = ensemble.predict(X_test_processed)\n",
        "\n",
        "# Metrics\n",
        "ensemble_auc = roc_auc_score(y_test, ensemble_probs)\n",
        "ensemble_logloss = log_loss(y_test, ensemble_probs)\n",
        "precision, recall, _ = precision_recall_curve(y_test, ensemble_probs)\n",
        "ensemble_pr_auc = auc(recall, precision)\n",
        "\n",
        "elapsed = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "results['Voting_Ensemble'] = {\n",
        "    'model': ensemble,\n",
        "    'best_params': 'N/A (Ensemble)',\n",
        "    'cv_auc': np.mean([results[name]['cv_auc'] for name, _ in top_3]),\n",
        "    'test_auc': ensemble_auc,\n",
        "    'test_logloss': ensemble_logloss,\n",
        "    'pr_auc': ensemble_pr_auc,\n",
        "    'test_probs': ensemble_probs,\n",
        "    'test_preds': ensemble_preds,\n",
        "    'train_time': elapsed\n",
        "}\n",
        "\n",
        "print(f\"‚úì Voting Ensemble AUC: {ensemble_auc:.4f}\")"
      ],
      "id": "voting-ensemble",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: results-table\n",
        "\n",
        "# ==============================================================================\n",
        "# TOURNAMENT RESULTS\n",
        "# ==============================================================================\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    name: {\n",
        "        'CV AUC': f\"{r['cv_auc']:.4f}\",\n",
        "        'Test AUC': f\"{r['test_auc']:.4f}\",\n",
        "        'PR AUC': f\"{r['pr_auc']:.4f}\",\n",
        "        'Log Loss': f\"{r['test_logloss']:.4f}\",\n",
        "        'Time (s)': f\"{r['train_time']:.1f}\"\n",
        "    }\n",
        "    for name, r in results.items()\n",
        "}).T\n",
        "\n",
        "results_df = results_df.sort_values('Test AUC', ascending=False)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üèÜ TOURNAMENT FINAL STANDINGS\")\n",
        "print(\"=\" * 70)\n",
        "print(results_df.to_string())\n",
        "\n",
        "# Best model\n",
        "best_model_name = results_df.index[0]\n",
        "best_result = results[best_model_name]\n",
        "print(f\"\\nü•á CHAMPION: {best_model_name} (Test AUC: {best_result['test_auc']:.4f})\")"
      ],
      "id": "results-table",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 7: Performance Visualization"
      ],
      "id": "e41d5b7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: roc-curves\n",
        "#| fig-cap: 'ROC & PR Curves: Model comparison across all candidates.'\n",
        "\n",
        "# ==============================================================================\n",
        "# ROC & PR CURVES\n",
        "# ==============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "colors = list(PROJECT_COLS.values())[:len(results)]\n",
        "\n",
        "# LEFT: ROC Curves\n",
        "ax1 = axes[0]\n",
        "for i, (name, r) in enumerate(sorted(results.items(), key=lambda x: -x[1]['test_auc'])):\n",
        "    fpr, tpr, _ = roc_curve(y_test, r['test_probs'])\n",
        "    linewidth = 3 if name == best_model_name else 1.5\n",
        "    alpha = 1.0 if name == best_model_name else 0.7\n",
        "    ax1.plot(fpr, tpr, label=f\"{name} ({r['test_auc']:.3f})\",\n",
        "             color=colors[i % len(colors)], linewidth=linewidth, alpha=alpha)\n",
        "\n",
        "ax1.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
        "ax1.set_xlabel('False Positive Rate')\n",
        "ax1.set_ylabel('True Positive Rate')\n",
        "ax1.set_title('ROC Curves', fontweight='bold')\n",
        "ax1.legend(loc='lower right', fontsize=9)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# RIGHT: PR Curves\n",
        "ax2 = axes[1]\n",
        "baseline = y_test.mean()\n",
        "for i, (name, r) in enumerate(sorted(results.items(), key=lambda x: -x[1]['pr_auc'])):\n",
        "    precision, recall, _ = precision_recall_curve(y_test, r['test_probs'])\n",
        "    linewidth = 3 if name == best_model_name else 1.5\n",
        "    alpha = 1.0 if name == best_model_name else 0.7\n",
        "    ax2.plot(recall, precision, label=f\"{name} ({r['pr_auc']:.3f})\",\n",
        "             color=colors[i % len(colors)], linewidth=linewidth, alpha=alpha)\n",
        "\n",
        "ax2.axhline(y=baseline, color='black', linestyle='--', linewidth=1, label=f'Baseline ({baseline:.3f})')\n",
        "ax2.set_xlabel('Recall')\n",
        "ax2.set_ylabel('Precision')\n",
        "ax2.set_title('Precision-Recall Curves', fontweight='bold')\n",
        "ax2.legend(loc='upper right', fontsize=9)\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "roc-curves",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 8: Revenue Curve & Business Lift"
      ],
      "id": "0860f187"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: revenue-calculation\n",
        "\n",
        "# ==============================================================================\n",
        "# REVENUE LIFT CALCULATION\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_revenue_curve(y_true, y_pred_proba, avg_deal=AVG_DEAL_SIZE, \n",
        "                            sql_to_deal=CONVERSION_TO_DEAL):\n",
        "    \"\"\"\n",
        "    Calculate cumulative revenue by leads contacted.\n",
        "    \n",
        "    Revenue = SQLs √ó SQL-to-Deal Rate √ó Average Deal Size\n",
        "    \"\"\"\n",
        "    df_rev = pd.DataFrame({\n",
        "        'actual': y_true.values,\n",
        "        'prob': y_pred_proba\n",
        "    }).sort_values('prob', ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    df_rev['cumulative_leads'] = range(1, len(df_rev) + 1)\n",
        "    df_rev['cumulative_sqls'] = df_rev['actual'].cumsum()\n",
        "    df_rev['cumulative_deals'] = df_rev['cumulative_sqls'] * sql_to_deal\n",
        "    df_rev['cumulative_revenue'] = df_rev['cumulative_deals'] * avg_deal\n",
        "    \n",
        "    # Random baseline\n",
        "    total_sqls = df_rev['actual'].sum()\n",
        "    df_rev['random_sqls'] = df_rev['cumulative_leads'] / len(df_rev) * total_sqls\n",
        "    df_rev['random_revenue'] = df_rev['random_sqls'] * sql_to_deal * avg_deal\n",
        "    \n",
        "    # Lift\n",
        "    df_rev['revenue_lift'] = df_rev['cumulative_revenue'] - df_rev['random_revenue']\n",
        "    \n",
        "    return df_rev\n",
        "\n",
        "\n",
        "def calculate_sales_differential(y_true, y_pred_proba, percentile=20):\n",
        "    \"\"\"Calculate additional SQLs at given percentile.\"\"\"\n",
        "    n_leads = len(y_true)\n",
        "    total_sqls = y_true.sum()\n",
        "    top_n = int(n_leads * percentile / 100)\n",
        "    \n",
        "    random_sqls = total_sqls * (percentile / 100)\n",
        "    \n",
        "    sorted_df = pd.DataFrame({\n",
        "        'actual': y_true.values,\n",
        "        'prob': y_pred_proba\n",
        "    }).sort_values('prob', ascending=False)\n",
        "    \n",
        "    model_sqls = sorted_df.head(top_n)['actual'].sum()\n",
        "    \n",
        "    return {\n",
        "        'percentile': percentile,\n",
        "        'leads_contacted': top_n,\n",
        "        'random_sqls': random_sqls,\n",
        "        'model_sqls': model_sqls,\n",
        "        'additional_sqls': model_sqls - random_sqls,\n",
        "        'lift_ratio': model_sqls / random_sqls if random_sqls > 0 else 0,\n",
        "        'model_revenue': model_sqls * CONVERSION_TO_DEAL * AVG_DEAL_SIZE,\n",
        "        'random_revenue': random_sqls * CONVERSION_TO_DEAL * AVG_DEAL_SIZE\n",
        "    }\n",
        "\n",
        "\n",
        "# Calculate for best model\n",
        "best_probs = results[best_model_name]['test_probs']\n",
        "revenue_df = calculate_revenue_curve(y_test, best_probs)\n",
        "\n",
        "# Print differential at key percentiles\n",
        "print(\"=\" * 70)\n",
        "print(f\"üí∞ REVENUE IMPACT ANALYSIS ({best_model_name})\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for pct in [10, 20, 30, 50]:\n",
        "    diff = calculate_sales_differential(y_test, best_probs, pct)\n",
        "    print(f\"\\nüìä Top {pct}% of Leads ({diff['leads_contacted']:,} leads):\")\n",
        "    print(f\"   Random SQLs: {diff['random_sqls']:.1f}\")\n",
        "    print(f\"   Model SQLs: {diff['model_sqls']:.0f}\")\n",
        "    print(f\"   ‚ûú Additional SQLs: +{diff['additional_sqls']:.1f}\")\n",
        "    print(f\"   ‚ûú Lift: {diff['lift_ratio']:.2f}x\")\n",
        "    print(f\"   ‚ûú Additional Revenue: ${(diff['model_revenue'] - diff['random_revenue']):,.0f}\")"
      ],
      "id": "revenue-calculation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: revenue-visualization\n",
        "#| fig-cap: 'Revenue Curve: Dollar-denominated impact of model-driven prioritization.'\n",
        "\n",
        "# ==============================================================================\n",
        "# REVENUE CURVE VISUALIZATION\n",
        "# ==============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# LEFT: Revenue Curve\n",
        "ax1 = axes[0]\n",
        "ax1.plot(revenue_df['cumulative_leads'], revenue_df['cumulative_revenue'] / 1e6,\n",
        "         color=PROJECT_COLS['Success'], linewidth=3, label='Model')\n",
        "ax1.plot(revenue_df['cumulative_leads'], revenue_df['random_revenue'] / 1e6,\n",
        "         color=PROJECT_COLS['Failure'], linewidth=2, linestyle='--', label='Random')\n",
        "ax1.fill_between(revenue_df['cumulative_leads'], \n",
        "                  revenue_df['random_revenue'] / 1e6,\n",
        "                  revenue_df['cumulative_revenue'] / 1e6,\n",
        "                  alpha=0.3, color=PROJECT_COLS['Success'], label='Revenue Lift')\n",
        "\n",
        "ax1.set_xlabel('Leads Contacted (Ranked by Score)', fontsize=12)\n",
        "ax1.set_ylabel('Cumulative Revenue ($M)', fontsize=12)\n",
        "ax1.set_title(f'Revenue Curve ({best_model_name})\\nModel vs Random Lead Selection',\n",
        "              fontweight='bold')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Add annotations\n",
        "top_20_idx = int(len(revenue_df) * 0.2)\n",
        "model_rev_20 = revenue_df.iloc[top_20_idx]['cumulative_revenue'] / 1e6\n",
        "random_rev_20 = revenue_df.iloc[top_20_idx]['random_revenue'] / 1e6\n",
        "ax1.axvline(x=top_20_idx, color='gray', linestyle=':', alpha=0.7)\n",
        "ax1.annotate(f'Top 20%\\n+${(model_rev_20 - random_rev_20):.2f}M',\n",
        "             xy=(top_20_idx, model_rev_20), xytext=(top_20_idx + 100, model_rev_20 + 0.1),\n",
        "             fontsize=10, arrowprops=dict(arrowstyle='->', color='gray'))\n",
        "\n",
        "# RIGHT: Lift by Decile\n",
        "ax2 = axes[1]\n",
        "\n",
        "# Calculate decile stats\n",
        "revenue_df['decile'] = pd.qcut(range(len(revenue_df)), 10, labels=False) + 1\n",
        "decile_stats = revenue_df.groupby('decile').agg({\n",
        "    'actual': ['sum', 'count'],\n",
        "    'prob': 'count'\n",
        "}).reset_index()\n",
        "decile_stats.columns = ['decile', 'sqls', 'leads', 'count']\n",
        "decile_stats['conversion'] = decile_stats['sqls'] / decile_stats['leads']\n",
        "decile_stats['revenue_per_lead'] = decile_stats['conversion'] * CONVERSION_TO_DEAL * AVG_DEAL_SIZE\n",
        "\n",
        "colors = [PROJECT_COLS['Success'] if i <= 3 else PROJECT_COLS['Neutral'] \n",
        "          for i in decile_stats['decile']]\n",
        "\n",
        "bars = ax2.bar(decile_stats['decile'], decile_stats['revenue_per_lead'],\n",
        "               color=colors, edgecolor='white', linewidth=1)\n",
        "\n",
        "avg_rev = (y_test.mean() * CONVERSION_TO_DEAL * AVG_DEAL_SIZE)\n",
        "ax2.axhline(y=avg_rev, color='red', linestyle='--', linewidth=2, label=f'Avg: ${avg_rev:,.0f}')\n",
        "\n",
        "ax2.set_xlabel('Decile (1 = Highest Scored)', fontsize=12)\n",
        "ax2.set_ylabel('Revenue per Lead ($)', fontsize=12)\n",
        "ax2.set_title('Expected Revenue per Lead by Decile', fontweight='bold')\n",
        "ax2.legend()\n",
        "\n",
        "for bar, rev in zip(bars, decile_stats['revenue_per_lead']):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
        "             f'${rev:,.0f}', ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "revenue-visualization",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 9: SHAP Interpretability"
      ],
      "id": "8cc8cf1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: shap-analysis\n",
        "#| fig-cap: 'SHAP Beeswarm: Feature impact on lead scoring predictions.'\n",
        "\n",
        "# ==============================================================================\n",
        "# SHAP BEESWARM PLOT\n",
        "# ==============================================================================\n",
        "\n",
        "if SHAP_AVAILABLE:\n",
        "    print(\"‚è≥ Computing SHAP values (this may take a moment)...\")\n",
        "    \n",
        "    # Use LightGBM or XGBoost for SHAP (best compatibility)\n",
        "    if 'LightGBM' in best_estimators:\n",
        "        shap_model = best_estimators['LightGBM']\n",
        "        shap_name = 'LightGBM'\n",
        "    elif 'XGBoost' in best_estimators:\n",
        "        shap_model = best_estimators['XGBoost']\n",
        "        shap_name = 'XGBoost'\n",
        "    else:\n",
        "        shap_model = best_estimators['Random_Forest']\n",
        "        shap_name = 'Random_Forest'\n",
        "    \n",
        "    # Create explainer\n",
        "    explainer = shap.TreeExplainer(shap_model)\n",
        "    \n",
        "    # Sample for speed\n",
        "    sample_size = min(500, len(X_test_processed))\n",
        "    sample_idx = np.random.choice(len(X_test_processed), sample_size, replace=False)\n",
        "    X_sample = X_test_processed[sample_idx]\n",
        "    \n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "    \n",
        "    # Handle different SHAP output formats\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = shap_values[1]  # Class 1 (Success)\n",
        "    \n",
        "    # Truncate feature names for display\n",
        "    display_names = [n[:35] + '...' if len(n) > 35 else n \n",
        "                     for n in FEATURE_NAMES[:X_sample.shape[1]]]\n",
        "    \n",
        "    # Beeswarm plot\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    shap.summary_plot(shap_values, X_sample,\n",
        "                      feature_names=display_names,\n",
        "                      show=False, max_display=20, plot_size=None)\n",
        "    plt.title(f'SHAP Feature Impact ({shap_name})\\nHow Features Push Scores Up/Down',\n",
        "              fontweight='bold', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Top features by importance\n",
        "    shap_importance = pd.DataFrame({\n",
        "        'feature': FEATURE_NAMES[:shap_values.shape[1]],\n",
        "        'importance': np.abs(shap_values).mean(axis=0)\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"üìä SHAP FEATURE IMPORTANCE ({shap_name})\")\n",
        "    print(\"=\" * 70)\n",
        "    print(shap_importance.head(15).to_string(index=False))\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è SHAP not available. Install with: pip install shap\")\n",
        "    \n",
        "    # Fallback: Feature importance from tree model\n",
        "    if 'LightGBM' in best_estimators:\n",
        "        model = best_estimators['LightGBM']\n",
        "        importance = model.feature_importances_\n",
        "    elif 'XGBoost' in best_estimators:\n",
        "        model = best_estimators['XGBoost']\n",
        "        importance = model.feature_importances_\n",
        "    else:\n",
        "        model = best_estimators['Random_Forest']\n",
        "        importance = model.feature_importances_\n",
        "    \n",
        "    imp_df = pd.DataFrame({\n",
        "        'feature': FEATURE_NAMES[:len(importance)],\n",
        "        'importance': importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(range(20), imp_df.head(20)['importance'].values[::-1],\n",
        "             color=PROJECT_COLS['Success'])\n",
        "    plt.yticks(range(20), imp_df.head(20)['feature'].values[::-1], fontsize=9)\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title('Feature Importance (Tree-Based)', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "shap-analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Phase 10: THE BOTTOM LINE"
      ],
      "id": "bbce132c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: bottom-line\n",
        "\n",
        "# ==============================================================================\n",
        "# THE BOTTOM LINE\n",
        "# ==============================================================================\n",
        "\n",
        "# Calculate monthly impact (scale from test set)\n",
        "monthly_scale = len(df_mx) / len(X_test) / 12  # Annualize and monthly\n",
        "\n",
        "top_20 = calculate_sales_differential(y_test, best_probs, 20)\n",
        "\n",
        "monthly_additional_sqls = top_20['additional_sqls'] * monthly_scale\n",
        "monthly_additional_revenue = monthly_additional_sqls * CONVERSION_TO_DEAL * AVG_DEAL_SIZE\n",
        "annual_additional_revenue = monthly_additional_revenue * 12\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"=\" * 70)\n",
        "print(\"                    üíé THE BOTTOM LINE üíé\")\n",
        "print(\"=\" * 70)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\"\"\n",
        "\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   üèÜ WINNING MODEL: {best_model_name:<45} ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   üìà TEST AUC-ROC: {best_result['test_auc']:.4f}                                         ‚ïë\n",
        "‚ïë   üìà PR-AUC:       {best_result['pr_auc']:.4f}                                         ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   By using the {best_model_name} model to prioritize                 ‚ïë\n",
        "‚ïë   the TOP 20% of Mx leads:                                           ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   ‚û§ ADDITIONAL SQLs/Month:     +{monthly_additional_sqls:,.0f}                             ‚ïë\n",
        "‚ïë   ‚û§ ADDITIONAL SQLs/Year:      +{monthly_additional_sqls * 12:,.0f}                            ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   ‚û§ LIFT vs Random:            {top_20['lift_ratio']:.2f}x                               ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   üí∞ ESTIMATED PIPELINE VALUE:                                       ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë      Monthly:   ${monthly_additional_revenue:>12,.0f}                            ‚ïë\n",
        "‚ïë      Annual:    ${annual_additional_revenue:>12,.0f}                            ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïë   (Assumes ${AVG_DEAL_SIZE:,} avg deal, {CONVERSION_TO_DEAL:.0%} SQL‚ÜíDeal)               ‚ïë\n",
        "‚ïë                                                                      ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"                    üìã RECOMMENDATIONS\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "  1. DEPLOY: Integrate lead scoring into Salesforce/HubSpot\n",
        "  \n",
        "  2. PRIORITIZE: Route top-decile Mx leads to senior reps\n",
        "  \n",
        "  3. NURTURE: Queue decile 4-6 leads for automated drip campaigns\n",
        "  \n",
        "  4. DISQUALIFY: Do not waste rep time on bottom 3 deciles\n",
        "  \n",
        "  5. MONITOR: Track actual vs predicted conversion monthly\n",
        "  \n",
        "  6. RETRAIN: Refresh model quarterly with new lead outcomes\n",
        "\"\"\")\n",
        "print(\"=\" * 70)"
      ],
      "id": "bottom-line",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: executive-dashboard\n",
        "#| fig-cap: 'Executive Dashboard: Complete model performance summary.'\n",
        "\n",
        "# ==============================================================================\n",
        "# EXECUTIVE SUMMARY DASHBOARD\n",
        "# ==============================================================================\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
        "\n",
        "# 1. Model Comparison Bar\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "model_aucs = [(name, r['test_auc']) for name, r in results.items()]\n",
        "model_aucs.sort(key=lambda x: x[1], reverse=True)\n",
        "colors = [PROJECT_COLS['Success'] if name == best_model_name else PROJECT_COLS['Neutral']\n",
        "          for name, _ in model_aucs]\n",
        "ax1.barh([m[0] for m in model_aucs], [m[1] for m in model_aucs], color=colors)\n",
        "ax1.set_xlabel('Test AUC')\n",
        "ax1.set_title('Model Comparison', fontweight='bold')\n",
        "ax1.set_xlim(0.5, 1.0)\n",
        "for i, (name, auc_val) in enumerate(model_aucs):\n",
        "    ax1.text(auc_val + 0.01, i, f'{auc_val:.4f}', va='center', fontsize=9)\n",
        "\n",
        "# 2. Cumulative Gain\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "gain_df = revenue_df.copy()\n",
        "gain_df['pct_leads'] = gain_df['cumulative_leads'] / len(gain_df) * 100\n",
        "gain_df['pct_sqls'] = gain_df['cumulative_sqls'] / gain_df['actual'].sum() * 100\n",
        "ax2.plot(gain_df['pct_leads'], gain_df['pct_sqls'], \n",
        "         color=PROJECT_COLS['Success'], linewidth=2, label='Model')\n",
        "ax2.plot([0, 100], [0, 100], 'k--', label='Random')\n",
        "ax2.fill_between(gain_df['pct_leads'], gain_df['pct_leads'], gain_df['pct_sqls'],\n",
        "                  alpha=0.3, color=PROJECT_COLS['Success'])\n",
        "ax2.set_xlabel('% Leads Contacted')\n",
        "ax2.set_ylabel('% SQLs Captured')\n",
        "ax2.set_title('Cumulative Gain', fontweight='bold')\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# 3. Lift by Percentile\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "pcts = [10, 20, 30, 40, 50]\n",
        "lifts = [calculate_sales_differential(y_test, best_probs, p)['lift_ratio'] for p in pcts]\n",
        "ax3.bar(pcts, lifts, color=PROJECT_COLS['Highlight'], width=8)\n",
        "ax3.axhline(y=1, color='black', linestyle='--')\n",
        "ax3.set_xlabel('Top X% of Leads')\n",
        "ax3.set_ylabel('Lift')\n",
        "ax3.set_title('Lift vs Random', fontweight='bold')\n",
        "for p, l in zip(pcts, lifts):\n",
        "    ax3.text(p, l + 0.05, f'{l:.2f}x', ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# 4. Score Distribution\n",
        "ax4 = fig.add_subplot(gs[1, 0])\n",
        "ax4.hist(best_probs[y_test == 0], bins=30, alpha=0.6, label='Fail',\n",
        "         color=PROJECT_COLS['Failure'], density=True)\n",
        "ax4.hist(best_probs[y_test == 1], bins=30, alpha=0.6, label='Success',\n",
        "         color=PROJECT_COLS['Success'], density=True)\n",
        "ax4.set_xlabel('Predicted Probability')\n",
        "ax4.set_ylabel('Density')\n",
        "ax4.set_title('Score Distribution', fontweight='bold')\n",
        "ax4.legend()\n",
        "\n",
        "# 5. Confusion Matrix\n",
        "ax5 = fig.add_subplot(gs[1, 1])\n",
        "cm = confusion_matrix(y_test, results[best_model_name]['test_preds'])\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax5,\n",
        "            xticklabels=['Pred Fail', 'Pred Success'],\n",
        "            yticklabels=['Actual Fail', 'Actual Success'])\n",
        "ax5.set_title(f'Confusion Matrix\\n({best_model_name})', fontweight='bold')\n",
        "\n",
        "# 6. Revenue Lift\n",
        "ax6 = fig.add_subplot(gs[1, 2])\n",
        "additional_revs = [(calculate_sales_differential(y_test, best_probs, p)['model_revenue'] -\n",
        "                    calculate_sales_differential(y_test, best_probs, p)['random_revenue']) / 1000\n",
        "                   for p in pcts]\n",
        "ax6.bar(pcts, additional_revs, color=PROJECT_COLS['Gold'], width=8)\n",
        "ax6.set_xlabel('Top X% of Leads')\n",
        "ax6.set_ylabel('Additional Revenue ($K)')\n",
        "ax6.set_title('Revenue Lift vs Random', fontweight='bold')\n",
        "for p, r in zip(pcts, additional_revs):\n",
        "    ax6.text(p, r + 1, f'${r:.0f}K', ha='center', fontsize=9, fontweight='bold')\n",
        "\n",
        "# 7-9. KPI Cards\n",
        "ax7 = fig.add_subplot(gs[2, :])\n",
        "ax7.axis('off')\n",
        "\n",
        "kpi_text = f\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï¶‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë     BEST MODEL        ‚ïë     TEST AUC          ‚ïë     TOP-20% LIFT      ‚ïë   ANNUAL REVENUE      ‚ïë\n",
        "‚ïë   {best_model_name:<18} ‚ïë       {best_result['test_auc']:.4f}          ‚ïë        {top_20['lift_ratio']:.2f}x           ‚ïë    ${annual_additional_revenue:>12,.0f}   ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï©‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\"\"\"\n",
        "\n",
        "ax7.text(0.5, 0.5, kpi_text, fontsize=13, fontfamily='monospace',\n",
        "         ha='center', va='center', transform=ax7.transAxes,\n",
        "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
        "\n",
        "plt.suptitle(f'MasterControl Mx Lead Scoring - {best_model_name}',\n",
        "             fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "executive-dashboard",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "# Appendix: Technical Methodology\n",
        "\n",
        "**Advanced Feature Engineering:**\n",
        "\n",
        "1.  **Latent Semantic Analysis (LSA):** We use TF-IDF vectorization followed by TruncatedSVD to reduce job titles to 15 dense semantic components. This captures latent meaning (e.g., \"Plant Manager\" and \"Factory Director\" map to similar vectors).\n",
        "\n",
        "2.  **Target Encoding:** For high-cardinality features like `acct_target_industry`, we use smoothed target encoding: $\\text{encoded} = \\frac{n \\cdot \\bar{y}_{category} + m \\cdot \\bar{y}_{global}}{n + m}$ where $m$ is the smoothing parameter.\n",
        "\n",
        "3.  **Polynomial Interactions:** We create explicit cross-product features for Seniority √ó Function to capture \"VP of Operations\" effects.\n",
        "\n",
        "**Hyperparameter Tuning:**\n",
        "\n",
        "-   GridSearchCV with 5-fold stratified CV\n",
        "-   XGBoost: `learning_rate`, `max_depth`, `scale_pos_weight`\n",
        "-   LightGBM: `num_leaves`, `learning_rate`, `scale_pos_weight`\n",
        "\n",
        "**Ensemble Strategy:**\n",
        "\n",
        "-   Soft Voting Classifier combining top 3 models\n",
        "-   Predictions averaged by probability, not hard votes\n",
        "\n",
        "**Business Metrics:**\n",
        "\n",
        "-   Revenue = SQLs √ó SQL-to-Deal Rate (12%) √ó Avg Deal Size (\\$50K)\n",
        "-   Lift = Model SQLs / Random SQLs at given percentile\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "*Model V2 generated for MSBA Capstone Case Competition - Spring 2026*"
      ],
      "id": "eec8f394"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "playwright_env",
      "language": "python",
      "display_name": "Python (playwright_env)",
      "path": "C:\\Users\\thoma\\AppData\\Roaming\\jupyter\\kernels\\playwright_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}