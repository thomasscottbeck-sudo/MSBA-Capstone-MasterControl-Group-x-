---
title: "The Revenue Engine V7: Domain-Optimized Titan"
subtitle: "Integrating Business Logic & 'Hidden Gem' Targeting"
author: "MSBA Capstone Group 3"
date: "Spring 2026"
format:
  html:
    theme: journal
    toc: true
    toc-depth: 3
    df-print: paged
    code-fold: true
    code-tools: true
  pdf:
    documentclass: article
    geometry: margin=1in
execute:
  echo: true
  warning: false
  message: false
---

# Executive Summary

**The Objective: Domain Mastery.** V6 proved we could model the data; V7 proves we understand the business. By encoding the "Hidden Gems" (Consultants), correcting for "Toxic Channels" (External Demand Gen), and calculating "Capital Density," V7 aligns the algorithm with the financial reality of the buyer.

**Strategic Discoveries:**

| Discovery | Insight | Impact |
|-----------|---------|--------|
| **The "Toxic" Channels** | 'External Demand Gen' and 'Email' leads account for 33% of volume but only ~4% conversion | Stop working these leads immediately |
| **The "Hidden Gems"** | Leads with "Not Enough Info" or "Non-manufacturing" actually convert at 30-46% | These are agile consultants/small firms that typical scoring misses |
| **Intent Hierarchy Error** | A "Webinar" P1 lead is actually lower intent than a generic lead | V7 corrects this classification error |
| **Capital Density** | Pharma/Bio leads carry 3x the budget potential | Weight scoring by industry budget proxy |

**What Changed from V6:**

| V6 Approach | V7 Titan Upgrade | Business Rationale |
|-------------|------------------|-------------------|
| Implicit channel treatment | `channel_efficiency` tiering (Premium/Standard/Toxic) | Identifies unprofitable lead sources |
| Missing data as negative | `is_hidden_gem` flag for "Unknown" accounts | Captures high-converting consultants |
| Uniform company sizing | `capital_density_score` industry-weighted | Budget proxy improves prioritization |
| Binary priority encoding | `intent_strength` ordinal mapping | Distinguishes true buyer urgency |
| Generic role matching | `role_product_match` alignment score | Routes leads to right product team |
| Single-word title scan | `title_bigrams` for key phrases | Captures "document control", "process engineer" |

------------------------------------------------------------------------

# Phase 1: Titan Environment Setup

**Why This Matters:** The V6 architecture proved robust. V7 retains the sklearn-compatible CatBoost wrapper and adds domain-specific feature engineering. We use **5-fold cross-validation** (n_iter=50) to balance compute budget with thorough search.

```{python}
#| label: setup
#| warning: false
#| message: false

# ==============================================================================
# PHASE 1: TITAN ENVIRONMENT
# ==============================================================================
# Domain-Optimized Stack with sklearn 1.6+ Compatibility

import subprocess
import sys

def install_if_missing(package_name, import_name=None, pip_name=None):
    """Install a package if not already available."""
    import_name = import_name or package_name.lower()
    pip_name = pip_name or import_name

    try:
        __import__(import_name)
        return True
    except ImportError:
        print(f"{package_name}: Not found. Installing...")
        try:
            subprocess.check_call(
                [sys.executable, "-m", "pip", "install", pip_name, "-q"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            print(f"{package_name}: Installed successfully!")
            return True
        except subprocess.CalledProcessError:
            print(f"{package_name}: Installation failed. Will use fallback.")
            return False

# ==============================================================================
# INSTALL DEPENDENCIES
# ==============================================================================
print("=" * 70)
print("V7 TITAN: CHECKING & INSTALLING DEPENDENCIES")
print("=" * 70)

install_if_missing("pandas")
install_if_missing("numpy")
install_if_missing("matplotlib")
install_if_missing("seaborn")
install_if_missing("scikit-learn", import_name="sklearn", pip_name="scikit-learn")
install_if_missing("pyprojroot", import_name="pyprojroot")
install_if_missing("CatBoost", import_name="catboost")
install_if_missing("XGBoost", import_name="xgboost")
install_if_missing("LightGBM", import_name="lightgbm")
install_if_missing("SHAP", import_name="shap")

print("=" * 70)

# ==============================================================================
# CORE IMPORTS
# ==============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import time
import re
import multiprocessing
from pathlib import Path
from datetime import datetime
from pyprojroot import here
from types import SimpleNamespace  # sklearn 1.6+ compatibility fix

warnings.filterwarnings('ignore')

# ==============================================================================
# PARALLELISM CONFIGURATION
# ==============================================================================
N_JOBS = multiprocessing.cpu_count() - 1
print(f"Parallelism: {N_JOBS} cores allocated (of {multiprocessing.cpu_count()} available)")

# Core ML
from sklearn.model_selection import (
    train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_predict
)
from sklearn.preprocessing import (
    StandardScaler, LabelEncoder, FunctionTransformer
)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone

# Metrics
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,
    classification_report, confusion_matrix, brier_score_loss, log_loss,
    f1_score, precision_score, recall_score
)

# Calibration
from sklearn.calibration import CalibratedClassifierCV, calibration_curve

# Ensemble & Stacking
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    StackingClassifier, VotingClassifier
)
from sklearn.linear_model import LogisticRegression

# ==============================================================================
# CATBOOST SKLEARN COMPATIBILITY WRAPPER (V6 FIX - RETAINED)
# ==============================================================================
# sklearn 1.6+ requires __sklearn_tags__ to return an object with dot-notation
# access. This wrapper uses SimpleNamespace to satisfy the requirement.

CATBOOST_AVAILABLE = False
CATBOOST_RAW_AVAILABLE = False

try:
    from catboost import CatBoostClassifier as CatBoostRaw
    CATBOOST_RAW_AVAILABLE = True
    print("CatBoost: Raw import successful")
except ImportError:
    print("CatBoost: Not available")

if CATBOOST_RAW_AVAILABLE:
    class SklearnCatBoost(BaseEstimator, ClassifierMixin):
        """
        sklearn-compatible CatBoost wrapper.
        Fixed for sklearn 1.6+ tag requirements using SimpleNamespace.
        """
        _estimator_type = "classifier"

        def __init__(self, iterations=500, depth=6, learning_rate=0.1,
                     l2_leaf_reg=3, border_count=64, random_state=42,
                     verbose=0, thread_count=1):  # thread_count=1 for safety
            self.iterations = iterations
            self.depth = depth
            self.learning_rate = learning_rate
            self.l2_leaf_reg = l2_leaf_reg
            self.border_count = border_count
            self.random_state = random_state
            self.verbose = verbose
            self.thread_count = thread_count
            self._model = None

        def __sklearn_tags__(self):
            """sklearn 1.6+ compatibility: Returns a namespace object."""
            tags = SimpleNamespace()
            tags.estimator_type = "classifier"
            tags.classifier_tags = SimpleNamespace()
            tags.regressor_tags = None
            tags.transformer_tags = None
            tags.input_tags = SimpleNamespace(
                allow_nan=True,
                pairwise=False,
                one_d_labels=True,
                two_d_labels=False
            )
            tags.target_tags = SimpleNamespace(
                required_y=True,
                one_d_labels=True,
                two_d_labels=False
            )
            return tags

        def fit(self, X, y, **fit_params):
            self._model = CatBoostRaw(
                iterations=self.iterations,
                depth=self.depth,
                learning_rate=self.learning_rate,
                l2_leaf_reg=self.l2_leaf_reg,
                border_count=self.border_count,
                random_state=self.random_state,
                verbose=self.verbose,
                thread_count=self.thread_count,
                allow_writing_files=False
            )
            self._model.fit(X, y, **fit_params)
            self.classes_ = np.unique(y)
            return self

        def predict(self, X):
            return self._model.predict(X).flatten().astype(int)

        def predict_proba(self, X):
            return self._model.predict_proba(X)

        @property
        def feature_importances_(self):
            return self._model.get_feature_importance()

    CATBOOST_AVAILABLE = True
    print("CatBoost: sklearn-compatible wrapper created")

# ==============================================================================
# OTHER BOOSTING LIBRARIES
# ==============================================================================

XGBOOST_AVAILABLE = False
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
    print("XGBoost: Ready")
except ImportError:
    print("XGBoost: Not available")

LIGHTGBM_AVAILABLE = False
try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
    print("LightGBM: Ready")
except ImportError:
    print("LightGBM: Not available")

# Target Encoding (sklearn 1.3+)
TARGET_ENCODER_AVAILABLE = False
try:
    from sklearn.preprocessing import TargetEncoder
    TARGET_ENCODER_AVAILABLE = True
    print("TargetEncoder: Ready (sklearn 1.3+)")
except ImportError:
    print("TargetEncoder: Not available (using manual implementation)")

# SHAP for explainability
SHAP_AVAILABLE = False
try:
    import shap
    SHAP_AVAILABLE = True
    print("SHAP: Ready")
except ImportError:
    print("SHAP: Not available")

# ==============================================================================
# PATH CONFIGURATION
# ==============================================================================
DATA_DIR = here("data")
OUTPUT_DIR = here("output")

CLEANED_DATA_PATH = here("output/Cleaned_QAL_Performance_for_MSBA.csv")
RAW_DATA_PATH = here("data/QAL Performance for MSBA.csv")

if CLEANED_DATA_PATH.exists():
    DATA_PATH = CLEANED_DATA_PATH
    print(f"\nUsing cleaned data: {CLEANED_DATA_PATH}")
else:
    DATA_PATH = RAW_DATA_PATH
    print(f"\nFallback to raw data: {RAW_DATA_PATH}")

# ==============================================================================
# HYPERPARAMETERS & CONFIGURATION (V7 TITAN)
# ==============================================================================
RANDOM_STATE = 42
CV_FOLDS = 5  # Robust validation
N_ITER_SEARCH = 50  # V7: Reduced from 100 to accommodate feature complexity
TEST_SIZE = 0.20
VAL_SIZE = 0.15

# Text Processing
LSA_COMPONENTS = 20
TFIDF_MAX_FEATURES = 500

# Business Parameters
COST_PER_CALL = 50
VALUE_PER_SQL = 6000

# SHAP Sampling
SHAP_BACKGROUND_SAMPLES = 100
SHAP_TEST_SAMPLES = 200

# Visual Configuration
PROJECT_COLS = {
    'Success': '#00534B',
    'Failure': '#F05627',
    'Neutral': '#95a5a6',
    'Highlight': '#2980b9',
    'Gold': '#f39c12',
    'Purple': '#9b59b6',
    'Profit': '#27ae60',
    'Toxic': '#e74c3c',
    'Premium': '#2ecc71'
}

sns.set_theme(style="whitegrid", context="talk")
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['axes.titleweight'] = 'bold'

print("\n" + "=" * 70)
print("V7 TITAN ENVIRONMENT INITIALIZED")
print("=" * 70)
print(f"Random State: {RANDOM_STATE}")
print(f"CV Folds: {CV_FOLDS}")
print(f"Search Iterations: {N_ITER_SEARCH}")
print(f"LSA Components: {LSA_COMPONENTS}")
print(f"Business: ${COST_PER_CALL} cost/call, ${VALUE_PER_SQL} value/SQL")
print(f"CatBoost sklearn-compatible: {CATBOOST_AVAILABLE}")

START_TIME = time.time()
```

------------------------------------------------------------------------

# Phase 2: Titan Domain Engineering

**The Strategic Breakthrough:** V7 doesn't just process data---it encodes **domain expertise**. Every feature below was discovered through forensic analysis of conversion patterns that V6 missed.

## 2.1 The Titan Feature Definitions

```{python}
#| label: titan-feature-definitions

# ==============================================================================
# V7 TITAN FEATURE MAPPINGS (Domain-Optimized)
# ==============================================================================

# -----------------------------------------------------------------------------
# 1. INTENT STRENGTH (Ordinal Encoding of Priority)
# -----------------------------------------------------------------------------
# Rationale: "Webinar" P1 leads are actually LOWER intent than "Contact Us" P1s.
# The current binary treatment loses this critical signal.

INTENT_STRENGTH_MAP = {
    'P1 - Website Pricing': 5,   # Highest intent: actively checking price
    'P1 - Contact Us': 5,        # Highest intent: direct outreach
    'P1 - Video Demo': 3,        # Medium-high: engaged but passive
    'P1 - Live Demo': 3,         # Medium-high: interested but needs convincing
    'P1 - Webinar Demo': 1,      # Low intent: passive learning (often "tire kickers")
    'No Priority': 1,            # Baseline
    'Priority 1': 2,             # Generic P1 without qualifier
    'Priority 2': 0              # Lowest priority
}

print("Intent Strength Mapping:")
for k, v in INTENT_STRENGTH_MAP.items():
    print(f"  {k}: {v}")

# -----------------------------------------------------------------------------
# 2. CHANNEL EFFICIENCY (Tiered Lead Source Quality)
# -----------------------------------------------------------------------------
# Rationale: 'External Demand Gen' converts at 2.5%. 'Direct/Inbound' converts
# at 18%+. Treating them equally is a modeling failure.

CHANNEL_TIER_MAP = {
    # Premium Channels (High conversion, high intent)
    'Direct/Inbound': 'Premium',
    'SEO': 'Premium',
    'Referrals': 'Premium',

    # Standard Channels (Acceptable conversion)
    'Online Ads': 'Standard',
    'Directory Listing': 'Standard',
    'Events': 'Standard',
    'Outbound Prospecting': 'Standard',

    # Toxic Channels (Volume without quality)
    'Email': 'Toxic',
    'External Demand Gen': 'Toxic'
}

CHANNEL_NUMERIC_MAP = {
    'Premium': 3,
    'Standard': 2,
    'Toxic': 1,
    'Unknown': 2  # Default to Standard
}

print("\nChannel Efficiency Tiers:")
for channel, tier in CHANNEL_TIER_MAP.items():
    print(f"  {channel} -> {tier}")

# -----------------------------------------------------------------------------
# 3. CAPITAL DENSITY SCORING (Budget Proxy)
# -----------------------------------------------------------------------------
# Rationale: A "Medium" Pharma company has 3x the budget of a "Medium"
# Consumer Packaged Goods company. Size alone is misleading.

INDUSTRY_BUDGET_MULTIPLIER = {
    'Pharma & BioTech': 3.0,
    'Blood & Biologics': 2.5,
    'Medical Device': 2.0,
    'Non-Life Science': 1.0,
    'Consumer Packaged Goods': 0.8
}

TIER_SIZE_MAP = {
    'Small': 50,
    'Medium': 500,
    'Large': 5000
}

print("\nCapital Density Components:")
print("  Industry Multipliers:", INDUSTRY_BUDGET_MULTIPLIER)
print("  Tier Size ($ proxy):", TIER_SIZE_MAP)

# -----------------------------------------------------------------------------
# 4. HIDDEN GEM IDENTIFICATION
# -----------------------------------------------------------------------------
# Rationale: "Not Enough Info Found" and "Non-manufacturing organization" leads
# convert at 30-46%. These are consultants, small firms, and agile buyers that
# traditional firmographic scoring misses entirely.

HIDDEN_GEM_SIGNALS = {
    'manufacturing_model': ['Not Enough Info Found'],
    'industry': ['Non-manufacturing organization']
}

print("\nHidden Gem Signals:")
print(f"  Manufacturing Model: {HIDDEN_GEM_SIGNALS['manufacturing_model']}")
print(f"  Industry: {HIDDEN_GEM_SIGNALS['industry']}")

# -----------------------------------------------------------------------------
# 5. ROLE-PRODUCT MATCH
# -----------------------------------------------------------------------------
# Rationale: A "Quality Manager" looking at "Qx" (Quality product) is a better
# fit than looking at "Mx" (Manufacturing product). Alignment = faster sales.

PRODUCT_ROLE_ALIGNMENT = {
    'Mx': ['Op', 'Mfg', 'Manuf', 'Production', 'Plant'],
    'Qx': ['Qual', 'QA', 'QC', 'Compliance', 'Validation']
}

print("\nRole-Product Alignment:")
for product, roles in PRODUCT_ROLE_ALIGNMENT.items():
    print(f"  {product}: {roles}")

# -----------------------------------------------------------------------------
# 6. HIGH-VALUE TITLE BIGRAMS
# -----------------------------------------------------------------------------
# Rationale: "Document Control" specialists are 2x more likely to convert.
# Single-word title parsing misses these compound phrases.

HIGH_VALUE_BIGRAMS = [
    'continuous improvement',
    'document control',
    'process engineer',
    'quality systems',
    'regulatory affairs',
    'quality assurance',
    'validation engineer',
    'compliance manager'
]

print("\nHigh-Value Title Bigrams:")
for bigram in HIGH_VALUE_BIGRAMS:
    print(f"  - '{bigram}'")
```

## 2.2 The Titan Feature Engineering Pipeline

```{python}
#| label: titan-pipeline

# ==============================================================================
# V7 TITAN: DOMAIN-OPTIMIZED DATA PIPELINE
# ==============================================================================

def clean_and_engineer_titan(filepath):
    """
    V7 Titan Data Pipeline: Domain-Optimized Feature Engineering.

    New Features:
    1. intent_strength - Ordinal encoding of priority (5=High, 0=Low)
    2. channel_efficiency - Tiered lead source (Premium/Standard/Toxic)
    3. is_hidden_gem - Flag for high-converting "Unknown" accounts
    4. capital_density_score - Industry-weighted budget proxy
    5. role_product_match - Product-title alignment score
    6. title_bigrams - Flags for high-value compound phrases
    """

    print("=" * 70)
    print("V7 TITAN: DOMAIN-OPTIMIZED FEATURE ENGINEERING")
    print("=" * 70)

    # Load data
    df = pd.read_csv(filepath)
    print(f"Loaded: {len(df):,} rows, {len(df.columns)} columns")

    # Standardize column names
    df.columns = [c.strip().lower().replace(' ', '_').replace('/', '_').replace('-', '_')
                  for c in df.columns]

    # Target variable
    if 'is_success' not in df.columns:
        success_stages = ['SQL', 'SQO', 'Won']
        df['is_success'] = df['next_stage__c'].isin(success_stages).astype(int)

    print(f"Target Rate: {df['is_success'].mean():.1%}")

    # =========================================================================
    # TITAN FEATURE 1: INTENT STRENGTH
    # =========================================================================
    print("\n[1/6] Engineering: intent_strength")

    if 'priority' in df.columns:
        df['intent_strength'] = df['priority'].map(INTENT_STRENGTH_MAP).fillna(1)

        # Validation
        intent_conv = df.groupby('intent_strength')['is_success'].agg(['mean', 'count'])
        print("  Intent Strength Conversion Rates:")
        for idx, row in intent_conv.iterrows():
            print(f"    Level {idx}: {row['mean']:.1%} (n={row['count']:,})")
    else:
        df['intent_strength'] = 1
        print("  [WARNING] 'priority' column not found. Defaulting to 1.")

    # =========================================================================
    # TITAN FEATURE 2: CHANNEL EFFICIENCY
    # =========================================================================
    print("\n[2/6] Engineering: channel_efficiency")

    channel_col = 'last_tactic_campaign_channel' if 'last_tactic_campaign_channel' in df.columns else 'lead_source'

    if channel_col in df.columns:
        # Map to tier
        df['channel_tier'] = df[channel_col].map(CHANNEL_TIER_MAP).fillna('Standard')
        df['channel_efficiency'] = df['channel_tier'].map(CHANNEL_NUMERIC_MAP)

        # Validation
        tier_conv = df.groupby('channel_tier')['is_success'].agg(['mean', 'count'])
        print("  Channel Tier Conversion Rates:")
        for tier in ['Premium', 'Standard', 'Toxic']:
            if tier in tier_conv.index:
                row = tier_conv.loc[tier]
                print(f"    {tier}: {row['mean']:.1%} (n={row['count']:,})")
    else:
        df['channel_tier'] = 'Standard'
        df['channel_efficiency'] = 2
        print(f"  [WARNING] Channel column not found. Defaulting to Standard.")

    # =========================================================================
    # TITAN FEATURE 3: HIDDEN GEM IDENTIFICATION
    # =========================================================================
    print("\n[3/6] Engineering: is_hidden_gem")

    model_col = 'acct_manufacturing_model' if 'acct_manufacturing_model' in df.columns else None
    industry_col = 'acct_target_industry' if 'acct_target_industry' in df.columns else None
    site_col = 'acct_primary_site_function' if 'acct_primary_site_function' in df.columns else None

    hidden_gem_mask = pd.Series(False, index=df.index)

    if model_col:
        # Check for "Not Enough Info Found" in manufacturing model
        hidden_gem_mask |= df[model_col].str.contains('Not Enough Info', case=False, na=False)

    if site_col:
        # Check for "Non-manufacturing organization" in site function
        hidden_gem_mask |= df[site_col].str.contains('Non-manufacturing', case=False, na=False)

    if industry_col:
        # Check for non-standard industries
        hidden_gem_mask |= df[industry_col].str.contains('Non-manufacturing', case=False, na=False)

    df['is_hidden_gem'] = hidden_gem_mask.astype(int)

    # Validation
    gem_conv = df.groupby('is_hidden_gem')['is_success'].agg(['mean', 'count'])
    print("  Hidden Gem Conversion Rates:")
    for idx, row in gem_conv.iterrows():
        label = "Hidden Gem" if idx == 1 else "Standard"
        print(f"    {label}: {row['mean']:.1%} (n={row['count']:,})")

    # =========================================================================
    # TITAN FEATURE 4: CAPITAL DENSITY SCORE
    # =========================================================================
    print("\n[4/6] Engineering: capital_density_score")

    tier_col = 'acct_tier_rollup' if 'acct_tier_rollup' in df.columns else None

    if industry_col and tier_col:
        # Get industry multiplier
        df['industry_multiplier'] = df[industry_col].map(
            lambda x: next((v for k, v in INDUSTRY_BUDGET_MULTIPLIER.items()
                           if k.lower() in str(x).lower()), 1.0)
        )

        # Get tier size
        df['tier_size'] = df[tier_col].map(TIER_SIZE_MAP).fillna(500)

        # Capital Density = Multiplier * Size
        df['capital_density_score'] = df['industry_multiplier'] * df['tier_size']

        # Log transform for normalization
        df['capital_density_log'] = np.log1p(df['capital_density_score'])

        # Clean up intermediate columns
        df = df.drop(columns=['industry_multiplier', 'tier_size'], errors='ignore')

        print(f"  Capital Density Range: {df['capital_density_score'].min():.0f} - {df['capital_density_score'].max():.0f}")
        print(f"  Capital Density Mean: {df['capital_density_score'].mean():.0f}")
    else:
        df['capital_density_score'] = 500
        df['capital_density_log'] = np.log1p(500)
        print("  [WARNING] Industry/Tier columns not found. Defaulting to 500.")

    # =========================================================================
    # TITAN FEATURE 5: ROLE-PRODUCT MATCH
    # =========================================================================
    print("\n[5/6] Engineering: role_product_match")

    title_col = 'contact_lead_title' if 'contact_lead_title' in df.columns else None
    product_col = 'product_segment' if 'product_segment' in df.columns else 'solution_rollup'

    if title_col and product_col in df.columns:
        def check_role_product_match(row):
            title = str(row[title_col]).lower() if pd.notna(row[title_col]) else ''
            product = str(row[product_col]) if pd.notna(row[product_col]) else ''

            if product in PRODUCT_ROLE_ALIGNMENT:
                keywords = PRODUCT_ROLE_ALIGNMENT[product]
                for kw in keywords:
                    if kw.lower() in title:
                        return 1
            return 0

        df['role_product_match'] = df.apply(check_role_product_match, axis=1)

        # Validation
        match_conv = df.groupby('role_product_match')['is_success'].agg(['mean', 'count'])
        print("  Role-Product Match Conversion:")
        for idx, row in match_conv.iterrows():
            label = "Matched" if idx == 1 else "Not Matched"
            print(f"    {label}: {row['mean']:.1%} (n={row['count']:,})")
    else:
        df['role_product_match'] = 0
        print("  [WARNING] Title/Product columns not found. Defaulting to 0.")

    # =========================================================================
    # TITAN FEATURE 6: TITLE BIGRAMS
    # =========================================================================
    print("\n[6/6] Engineering: title_bigrams")

    if title_col and title_col in df.columns:
        for bigram in HIGH_VALUE_BIGRAMS:
            col_name = 'has_' + bigram.replace(' ', '_')
            df[col_name] = df[title_col].str.lower().str.contains(bigram, na=False).astype(int)

        # Summary count
        bigram_cols = [c for c in df.columns if c.startswith('has_')]
        df['title_bigram_count'] = df[bigram_cols].sum(axis=1)

        print(f"  Created {len(bigram_cols)} bigram flags")
        print(f"  Leads with 1+ bigram: {(df['title_bigram_count'] > 0).sum():,}")
    else:
        df['title_bigram_count'] = 0
        print("  [WARNING] Title column not found. Skipping bigrams.")

    # =========================================================================
    # RETAINED V6 FEATURES
    # =========================================================================
    print("\n" + "-" * 50)
    print("RETAINING V6 FEATURES")
    print("-" * 50)

    # Product segmentation
    if 'product_segment' not in df.columns:
        def segment_product(sol):
            if str(sol) == 'Mx': return 'Mx'
            elif str(sol) == 'Qx': return 'Qx'
            return 'Other'
        df['product_segment'] = df['solution_rollup'].apply(segment_product)

    # Title parsing (Seniority, Function, Scope)
    if 'title_seniority' not in df.columns and title_col and title_col in df.columns:
        def parse_seniority(t):
            if pd.isna(t): return 'Unknown'
            t = str(t).lower()
            if re.search(r'\b(ceo|cfo|coo|cto|cio|chief|c-level|president)\b', t): return 'C-Suite'
            if re.search(r'\b(svp|senior vice president|evp)\b', t): return 'SVP'
            if re.search(r'\b(vp|vice president)\b', t): return 'VP'
            if re.search(r'\b(director|head of)\b', t): return 'Director'
            if re.search(r'\b(manager|mgr|supervisor|lead)\b', t): return 'Manager'
            if re.search(r'\b(analyst|engineer|specialist|associate|coordinator)\b', t): return 'IC'
            return 'Other'

        def parse_function(t):
            if pd.isna(t): return 'Unknown'
            t = str(t).lower()
            if re.search(r'\b(quality|qa|qc|qms|compliance|validation|capa)\b', t): return 'Quality'
            if re.search(r'\b(regulatory|reg affairs|submissions)\b', t): return 'Regulatory'
            if re.search(r'\b(manufacturing|production|operations|ops|plant|supply)\b', t): return 'Mfg/Ops'
            if re.search(r'\b(it|information tech|software|systems|data)\b', t): return 'IT'
            if re.search(r'\b(r&d|research|development|scientist|clinical|lab)\b', t): return 'R&D'
            if re.search(r'\b(project|program|pmo)\b', t): return 'PMO'
            return 'Other'

        def parse_scope(t):
            if pd.isna(t): return 'Standard'
            t = str(t).lower()
            if re.search(r'\b(global|worldwide|international|corporate|enterprise)\b', t): return 'Global'
            if re.search(r'\b(regional|division|group)\b', t): return 'Regional'
            if re.search(r'\b(site|plant|facility|local)\b', t): return 'Site'
            return 'Standard'

        df['title_seniority'] = df[title_col].apply(parse_seniority)
        df['title_function'] = df[title_col].apply(parse_function)
        df['title_scope'] = df[title_col].apply(parse_scope)

    # Decision maker flag
    if 'is_decision_maker' not in df.columns:
        df['is_decision_maker'] = df['title_seniority'].isin(['C-Suite', 'SVP', 'VP', 'Director']).astype(int)

    # Temporal features
    if 'cohort_date' in df.columns or 'qal_cohort_date' in df.columns:
        cohort_col = 'qal_cohort_date' if 'qal_cohort_date' in df.columns else 'cohort_date'
        df['cohort_date'] = pd.to_datetime(df[cohort_col], errors='coerce')
        if 'lead_age_days' not in df.columns:
            snapshot_date = df['cohort_date'].max()
            df['lead_age_days'] = (snapshot_date - df['cohort_date']).dt.days

    # Velocity tiers
    if 'lead_age_days' in df.columns:
        df['velocity_tier'] = pd.cut(
            df['lead_age_days'].fillna(0),
            bins=[-1, 30, 60, 90, 180, 9999],
            labels=['Hot', 'Warm', 'Cooling', 'Cold', 'Stale']
        ).astype(str)
        df['is_fresh'] = (df['lead_age_days'] <= 30).astype(int)
        df['is_stale'] = (df['lead_age_days'] > 180).astype(int)

    # Power Trio interactions (V6)
    seniority_col = 'title_seniority' if 'title_seniority' in df.columns else None
    industry_col = 'acct_target_industry' if 'acct_target_industry' in df.columns else None
    model_col = 'acct_manufacturing_model' if 'acct_manufacturing_model' in df.columns else None

    if seniority_col and industry_col and model_col:
        df['seniority_x_industry'] = df[seniority_col].astype(str) + '_' + df[industry_col].astype(str)
        df['seniority_x_model'] = df[seniority_col].astype(str) + '_' + df[model_col].astype(str)
        df['industry_x_model'] = df[industry_col].astype(str) + '_' + df[model_col].astype(str)
        df['power_trio'] = (df[seniority_col].astype(str) + '_' +
                           df[industry_col].astype(str) + '_' +
                           df[model_col].astype(str))

    # Golden Segment (V6)
    if seniority_col and industry_col and model_col:
        senior_mask = df[seniority_col].isin(['Director', 'VP', 'SVP', 'C-Suite'])
        pharma_mask = df[industry_col].str.contains('Pharma|Life|Bio', case=False, na=False)
        inhouse_mask = df[model_col].str.contains('In-House|In House|Inhouse', case=False, na=False)
        df['is_golden_segment'] = (senior_mask & pharma_mask & inhouse_mask).astype(int)
        df['is_senior_pharma'] = (senior_mask & pharma_mask).astype(int)

    # Global scope indicator
    if 'title_scope' in df.columns:
        df['is_global_scope'] = (df['title_scope'] == 'Global').astype(int)

    # Fill missing categoricals
    categorical_cols = ['acct_manufacturing_model', 'acct_primary_site_function',
                        'acct_target_industry', 'acct_territory_rollup',
                        'title_seniority', 'title_function', 'title_scope',
                        'channel_tier']

    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].fillna('Unknown')

    # =========================================================================
    # SUMMARY
    # =========================================================================
    print("\n" + "=" * 70)
    print("V7 TITAN FEATURE ENGINEERING COMPLETE")
    print("=" * 70)

    titan_features = ['intent_strength', 'channel_efficiency', 'is_hidden_gem',
                      'capital_density_score', 'capital_density_log',
                      'role_product_match', 'title_bigram_count']
    titan_features = [f for f in titan_features if f in df.columns]

    print(f"New Titan Features: {titan_features}")
    print(f"Total columns: {len(df.columns)}")

    return df

# Execute pipeline
df = clean_and_engineer_titan(DATA_PATH)
```

## 2.3 Titan Feature Correlation Analysis

```{python}
#| label: titan-correlation

# ==============================================================================
# TITAN FEATURE CORRELATION WITH TARGET
# ==============================================================================

print("\n" + "=" * 70)
print("TITAN FEATURE CORRELATION ANALYSIS")
print("=" * 70)

# Select Titan features for correlation
titan_numeric_features = [
    'intent_strength', 'channel_efficiency', 'is_hidden_gem',
    'capital_density_log', 'role_product_match', 'title_bigram_count',
    'is_golden_segment', 'is_decision_maker', 'is_fresh', 'is_stale',
    'is_global_scope', 'lead_age_days'
]

titan_numeric_features = [f for f in titan_numeric_features if f in df.columns]

# Calculate correlations with target
correlations = df[titan_numeric_features + ['is_success']].corr()['is_success'].drop('is_success')
correlations = correlations.sort_values(ascending=False)

print("\nFeature Correlations with is_success:")
print("-" * 40)
for feat, corr in correlations.items():
    direction = "+" if corr > 0 else "-"
    print(f"  {feat:30s}: {direction}{abs(corr):.4f}")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Panel 1: Correlation Bar Chart
ax1 = axes[0]
colors = [PROJECT_COLS['Success'] if c > 0 else PROJECT_COLS['Failure'] for c in correlations.values]
correlations.plot(kind='barh', ax=ax1, color=colors)
ax1.axvline(x=0, color='black', linewidth=1)
ax1.set_xlabel('Correlation with is_success')
ax1.set_title('Titan Feature Correlations', fontweight='bold')

# Panel 2: Channel Tier Conversion Comparison
ax2 = axes[1]
channel_conv = df.groupby('channel_tier')['is_success'].mean().reindex(['Premium', 'Standard', 'Toxic'])
tier_colors = [PROJECT_COLS['Premium'], PROJECT_COLS['Neutral'], PROJECT_COLS['Toxic']]
channel_conv.plot(kind='bar', ax=ax2, color=tier_colors, edgecolor='black')
ax2.set_ylabel('Conversion Rate')
ax2.set_xlabel('Channel Tier')
ax2.set_title('Conversion by Channel Tier', fontweight='bold')
ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)

# Add value labels
for i, v in enumerate(channel_conv):
    ax2.text(i, v + 0.005, f'{v:.1%}', ha='center', fontweight='bold')

plt.tight_layout()
plt.show()

# Print key insights
print("\n" + "=" * 70)
print("KEY TITAN INSIGHTS")
print("=" * 70)

if 'channel_tier' in df.columns:
    premium_rate = df[df['channel_tier'] == 'Premium']['is_success'].mean()
    toxic_rate = df[df['channel_tier'] == 'Toxic']['is_success'].mean()
    print(f"Premium Channel Conversion: {premium_rate:.1%}")
    print(f"Toxic Channel Conversion: {toxic_rate:.1%}")
    print(f"Lift from Premium vs Toxic: {premium_rate/toxic_rate:.1f}x")

if 'is_hidden_gem' in df.columns:
    gem_rate = df[df['is_hidden_gem'] == 1]['is_success'].mean()
    baseline_rate = df['is_success'].mean()
    print(f"\nHidden Gem Conversion: {gem_rate:.1%}")
    print(f"Baseline Conversion: {baseline_rate:.1%}")
    print(f"Hidden Gem Lift: {gem_rate/baseline_rate:.1f}x")
```

## 2.4 Feature Matrix Preparation

```{python}
#| label: feature-matrix

# ==============================================================================
# FEATURE MATRIX CONSTRUCTION
# ==============================================================================

def prepare_feature_matrix(df):
    """Prepare the feature matrix for modeling with Titan features."""

    print("\n" + "=" * 70)
    print("FEATURE MATRIX PREPARATION")
    print("=" * 70)

    y = df['is_success'].values

    # Categorical features
    categorical_features = [
        'title_seniority', 'title_function', 'title_scope',
        'acct_target_industry', 'acct_manufacturing_model',
        'acct_primary_site_function', 'acct_territory_rollup',
        'product_segment', 'channel_tier'  # V7: Added channel_tier
    ]

    # Interaction features
    interaction_features = [
        'seniority_x_industry', 'seniority_x_model', 'industry_x_model',
        'power_trio'
    ]

    # Velocity categorical
    velocity_cats = ['velocity_tier']

    # Filter to existing
    categorical_features = [c for c in categorical_features if c in df.columns]
    interaction_features = [c for c in interaction_features if c in df.columns]
    velocity_cats = [c for c in velocity_cats if c in df.columns]

    all_categoricals = categorical_features + interaction_features + velocity_cats

    # Numeric features (including V7 Titan Features)
    numeric_features = [
        'lead_age_days', 'is_decision_maker', 'is_fresh', 'is_stale',
        # V6 Golden Features
        'is_golden_segment', 'is_senior_pharma', 'is_global_scope',
        # V7 TITAN Features
        'intent_strength', 'channel_efficiency', 'is_hidden_gem',
        'capital_density_log', 'role_product_match', 'title_bigram_count'
    ]

    # Add bigram flags
    bigram_cols = [c for c in df.columns if c.startswith('has_')]
    numeric_features.extend(bigram_cols)

    if 'record_completeness' in df.columns:
        numeric_features.append('record_completeness')

    numeric_features = [c for c in numeric_features if c in df.columns]

    # Text feature
    text_col = 'contact_lead_title' if 'contact_lead_title' in df.columns else None

    # Build feature dataframe
    X = df[all_categoricals + numeric_features].copy()
    text_data = df[text_col].fillna('') if text_col else None

    print(f"Categorical features: {len(all_categoricals)}")
    print(f"  Base: {categorical_features}")
    print(f"  Interactions: {interaction_features}")
    print(f"Numeric features: {len(numeric_features)}")

    titan_nums = [c for c in numeric_features if c in
                  ['intent_strength', 'channel_efficiency', 'is_hidden_gem',
                   'capital_density_log', 'role_product_match', 'title_bigram_count']]
    print(f"  V7 Titan: {titan_nums}")

    print(f"Text feature: {text_col}")

    return X, y, text_data, all_categoricals, numeric_features

X, y, text_data, cat_cols, num_cols = prepare_feature_matrix(df)
```

## 2.5 Train/Validation/Test Split & Encoding

```{python}
#| label: data-split

# ==============================================================================
# DATA SPLITTING & TARGET ENCODING
# ==============================================================================

print("\n" + "=" * 70)
print("DATA SPLITTING & TARGET ENCODING")
print("=" * 70)

# Stratified split
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=VAL_SIZE/(1-TEST_SIZE), random_state=RANDOM_STATE, stratify=y_temp
)

# Split text data
if text_data is not None:
    text_temp, text_test = train_test_split(
        text_data, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
    )
    text_train, text_val = train_test_split(
        text_temp, test_size=VAL_SIZE/(1-TEST_SIZE), random_state=RANDOM_STATE, stratify=y_temp
    )
else:
    text_train = text_val = text_test = None

print(f"Train: {len(X_train):,} ({y_train.mean():.1%} positive)")
print(f"Val:   {len(X_val):,} ({y_val.mean():.1%} positive)")
print(f"Test:  {len(X_test):,} ({y_test.mean():.1%} positive)")

# ==============================================================================
# TARGET ENCODING
# ==============================================================================

print("\nApplying Target Encoding to high-cardinality features...")

target_encode_cols = [c for c in cat_cols if X_train[c].nunique() > 10]
standard_encode_cols = [c for c in cat_cols if c not in target_encode_cols]

print(f"  Target-encoded ({len(target_encode_cols)}): {target_encode_cols}")
print(f"  Label-encoded ({len(standard_encode_cols)}): {standard_encode_cols}")

# Manual Target Encoder (fallback)
class ManualTargetEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None, smoothing=10):
        self.columns = columns
        self.smoothing = smoothing
        self.encoding_maps_ = {}
        self.global_mean_ = None

    def fit(self, X, y):
        X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X
        y = np.array(y)
        self.global_mean_ = y.mean()
        cols_to_encode = self.columns if self.columns else X.select_dtypes(include=['object', 'category']).columns.tolist()
        for col in cols_to_encode:
            if col in X.columns:
                df_temp = pd.DataFrame({'col': X[col].astype(str), 'target': y})
                agg = df_temp.groupby('col')['target'].agg(['mean', 'count'])
                smoothed = (agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) / (agg['count'] + self.smoothing)
                self.encoding_maps_[col] = smoothed.to_dict()
        return self

    def transform(self, X):
        X = pd.DataFrame(X).copy() if not isinstance(X, pd.DataFrame) else X.copy()
        for col, mapping in self.encoding_maps_.items():
            if col in X.columns:
                X[col + '_encoded'] = X[col].astype(str).map(mapping).fillna(self.global_mean_)
        return X

# Apply Target Encoding
if TARGET_ENCODER_AVAILABLE and len(target_encode_cols) > 0:
    target_encoder = TargetEncoder(smooth='auto', target_type='binary')

    X_train_te = X_train.copy()
    X_val_te = X_val.copy()
    X_test_te = X_test.copy()

    te_train = target_encoder.fit_transform(X_train[target_encode_cols], y_train)
    te_val = target_encoder.transform(X_val[target_encode_cols])
    te_test = target_encoder.transform(X_test[target_encode_cols])

    for i, col in enumerate(target_encode_cols):
        X_train_te[col] = te_train[:, i]
        X_val_te[col] = te_val[:, i]
        X_test_te[col] = te_test[:, i]

elif len(target_encode_cols) > 0:
    manual_encoder = ManualTargetEncoder(columns=target_encode_cols, smoothing=10)

    X_train_te = manual_encoder.fit_transform(X_train, y_train)
    X_val_te = manual_encoder.transform(X_val)
    X_test_te = manual_encoder.transform(X_test)

    for col in target_encode_cols:
        if col + '_encoded' in X_train_te.columns:
            X_train_te[col] = X_train_te[col + '_encoded']
            X_val_te[col] = X_val_te[col + '_encoded']
            X_test_te[col] = X_test_te[col + '_encoded']
else:
    X_train_te = X_train.copy()
    X_val_te = X_val.copy()
    X_test_te = X_test.copy()

# ==============================================================================
# LABEL ENCODING
# ==============================================================================

label_encoders = {}
for col in standard_encode_cols:
    le = LabelEncoder()
    X_train_te[col] = le.fit_transform(X_train_te[col].astype(str))

    def safe_transform(series, encoder):
        return series.astype(str).apply(
            lambda x: encoder.transform([x])[0] if x in encoder.classes_ else 0
        )

    X_val_te[col] = safe_transform(X_val_te[col], le)
    X_test_te[col] = safe_transform(X_test_te[col], le)
    label_encoders[col] = le

# ==============================================================================
# DEEP LSA FOR TEXT
# ==============================================================================

if text_train is not None:
    print(f"\nApplying Deep LSA ({LSA_COMPONENTS} components)...")

    tfidf = TfidfVectorizer(
        max_features=TFIDF_MAX_FEATURES,
        ngram_range=(1, 2),
        stop_words='english',
        min_df=5
    )

    tfidf_train = tfidf.fit_transform(text_train)
    tfidf_val = tfidf.transform(text_val)
    tfidf_test = tfidf.transform(text_test)

    svd = TruncatedSVD(n_components=LSA_COMPONENTS, random_state=RANDOM_STATE)

    lsa_train = svd.fit_transform(tfidf_train)
    lsa_val = svd.transform(tfidf_val)
    lsa_test = svd.transform(tfidf_test)

    print(f"  Explained variance: {svd.explained_variance_ratio_.sum():.1%}")

    lsa_cols = [f'lsa_{i}' for i in range(LSA_COMPONENTS)]

    for i, col in enumerate(lsa_cols):
        X_train_te[col] = lsa_train[:, i]
        X_val_te[col] = lsa_val[:, i]
        X_test_te[col] = lsa_test[:, i]

# ==============================================================================
# FINAL NUMERIC CONVERSION
# ==============================================================================

for col in X_train_te.columns:
    if X_train_te[col].dtype == 'object':
        le = LabelEncoder()
        X_train_te[col] = le.fit_transform(X_train_te[col].astype(str))

        def safe_encode(series, encoder):
            return series.astype(str).apply(
                lambda x: encoder.transform([x])[0] if x in encoder.classes_ else 0
            )

        X_val_te[col] = safe_encode(X_val_te[col], le)
        X_test_te[col] = safe_encode(X_test_te[col], le)

X_train_te = X_train_te.fillna(0)
X_val_te = X_val_te.fillna(0)
X_test_te = X_test_te.fillna(0)

print(f"\nFinal feature matrix shape: {X_train_te.shape}")
print(f"Features: {list(X_train_te.columns)}")
```

------------------------------------------------------------------------

# Phase 3: Deep Model Tournament

**The Titan Approach:** V7 retains the platinum architecture (CatBoost, Stacking, Deep Search) with the **n_iter=50** search to accommodate increased feature complexity while maintaining robust 5-fold cross-validation.

## 3.1 Base Model Definitions

```{python}
#| label: model-definitions

# ==============================================================================
# PHASE 3: TITAN MODEL TOURNAMENT
# ==============================================================================

print("\n" + "=" * 70)
print("TITAN MODEL TOURNAMENT")
print("=" * 70)

models = {}
param_grids = {}

pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
print(f"Class imbalance ratio: {pos_weight:.2f}")

# -----------------------------------------------------------------------------
# 1. CATBOOST (sklearn-compatible wrapper)
# -----------------------------------------------------------------------------
if CATBOOST_AVAILABLE:
    models['CatBoost'] = SklearnCatBoost(
        random_state=RANDOM_STATE,
        verbose=0,
        thread_count=1  # Single-threaded for nested parallelism safety
    )
    param_grids['CatBoost'] = {
        'depth': [4, 6, 8, 10],
        'learning_rate': [0.01, 0.03, 0.05, 0.1],
        'iterations': [300, 500, 800],
        'l2_leaf_reg': [1, 3, 5, 7],
        'border_count': [32, 64, 128]
    }
    print("CatBoost: Configured with sklearn-compatible wrapper (Thread-Safe)")

# -----------------------------------------------------------------------------
# 2. XGBOOST
# -----------------------------------------------------------------------------
if XGBOOST_AVAILABLE:
    models['XGBoost'] = XGBClassifier(
        random_state=RANDOM_STATE,
        n_jobs=1,
        eval_metric='logloss'
    )
    param_grids['XGBoost'] = {
        'max_depth': [4, 6, 8, 10],
        'learning_rate': [0.01, 0.03, 0.05, 0.1],
        'n_estimators': [300, 500, 800],
        'scale_pos_weight': [1, pos_weight],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [1, 2, 5]
    }
    print("XGBoost: Configured with expanded search space")

# -----------------------------------------------------------------------------
# 3. LIGHTGBM
# -----------------------------------------------------------------------------
if LIGHTGBM_AVAILABLE:
    models['LightGBM'] = LGBMClassifier(
        random_state=RANDOM_STATE,
        n_jobs=1,
        verbose=-1
    )
    param_grids['LightGBM'] = {
        'num_leaves': [31, 63, 127, 255],
        'learning_rate': [0.01, 0.03, 0.05, 0.1],
        'n_estimators': [300, 500, 800],
        'class_weight': ['balanced', None],
        'subsample': [0.7, 0.8, 0.9, 1.0],
        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [1, 2, 5]
    }
    print("LightGBM: Configured with expanded search space")

# -----------------------------------------------------------------------------
# 4. GRADIENT BOOSTING (Fallback)
# -----------------------------------------------------------------------------
models['GradientBoosting'] = GradientBoostingClassifier(
    random_state=RANDOM_STATE
)
param_grids['GradientBoosting'] = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0]
}
print("GradientBoosting: Configured as fallback")

# -----------------------------------------------------------------------------
# 5. RANDOM FOREST
# -----------------------------------------------------------------------------
models['RandomForest'] = RandomForestClassifier(
    random_state=RANDOM_STATE,
    n_jobs=1,
    class_weight='balanced'
)
param_grids['RandomForest'] = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
print("RandomForest: Configured with balanced class weights")

print(f"\nTotal models in tournament: {len(models)}")
print(f"Search iterations per model: {N_ITER_SEARCH}")
print(f"Cross-validation folds: {CV_FOLDS}")
```

## 3.2 Hyperparameter Search

```{python}
#| label: hyperparameter-search

# ==============================================================================
# RANDOMIZED SEARCH (n_iter=50, cv=5)
# ==============================================================================

print("\n" + "=" * 70)
print("TITAN HYPERPARAMETER OPTIMIZATION (n_iter=50, cv=5)")
print("=" * 70)

cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)

best_models = {}
cv_results = {}

for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"Tuning: {name}")
    print(f"{'='*50}")

    search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_grids[name],
        n_iter=N_ITER_SEARCH,
        cv=cv,
        scoring='roc_auc',
        n_jobs=N_JOBS,
        random_state=RANDOM_STATE,
        verbose=1
    )

    search.fit(X_train_te, y_train)

    best_models[name] = search.best_estimator_
    cv_results[name] = {
        'best_score': search.best_score_,
        'best_params': search.best_params_,
        'cv_results': search.cv_results_
    }

    print(f"Best CV AUC: {search.best_score_:.4f}")
    print(f"Best params: {search.best_params_}")

# ==============================================================================
# VALIDATION SET EVALUATION
# ==============================================================================

print("\n" + "=" * 70)
print("VALIDATION SET PERFORMANCE")
print("=" * 70)

val_results = {}
for name, model in best_models.items():
    probs = model.predict_proba(X_val_te)[:, 1]
    auc = roc_auc_score(y_val, probs)
    val_results[name] = {'auc': auc, 'probs': probs}
    print(f"{name}: AUC = {auc:.4f}")

val_ranking = sorted(val_results.items(), key=lambda x: x[1]['auc'], reverse=True)
print(f"\nValidation Ranking:")
for i, (name, res) in enumerate(val_ranking, 1):
    print(f"  {i}. {name}: {res['auc']:.4f}")

# Print tournament results for PDF
print("\n\nMODEL TOURNAMENT RESULTS (For PDF Extraction):")
tournament_df = pd.DataFrame([
    {'Model': name, 'CV AUC': f"{cv_results[name]['best_score']:.4f}",
     'Val AUC': f"{val_results[name]['auc']:.4f}"}
    for name in best_models.keys()
]).sort_values('Val AUC', ascending=False)
print(tournament_df.to_markdown(index=False))
```

## 3.3 Stacking Ensemble

```{python}
#| label: stacking-ensemble

# ==============================================================================
# STACKING ENSEMBLE
# ==============================================================================

print("\n" + "=" * 70)
print("STACKING ENSEMBLE CONSTRUCTION")
print("=" * 70)

top_3_names = [name for name, _ in val_ranking[:3]]
print(f"Base learners: {top_3_names}")

stacking_estimators = [(name, best_models[name]) for name in top_3_names]

meta_learner = LogisticRegression(
    random_state=RANDOM_STATE,
    max_iter=1000,
    class_weight='balanced'
)

stacking_clf = StackingClassifier(
    estimators=stacking_estimators,
    final_estimator=meta_learner,
    cv=CV_FOLDS,
    stack_method='predict_proba',
    n_jobs=N_JOBS,
    passthrough=False
)

print("Training Stacking Ensemble...")
stacking_clf.fit(X_train_te, y_train)

stack_val_probs = stacking_clf.predict_proba(X_val_te)[:, 1]
stack_val_auc = roc_auc_score(y_val, stack_val_probs)

print(f"\nStacking Ensemble Validation AUC: {stack_val_auc:.4f}")

best_individual_auc = val_ranking[0][1]['auc']
improvement = stack_val_auc - best_individual_auc
print(f"Improvement over best individual ({val_ranking[0][0]}): {improvement:+.4f}")

best_models['StackingEnsemble'] = stacking_clf
val_results['StackingEnsemble'] = {'auc': stack_val_auc, 'probs': stack_val_probs}
```

## 3.4 Final Model Selection & Test Evaluation

```{python}
#| label: final-evaluation

# ==============================================================================
# FINAL TEST SET EVALUATION
# ==============================================================================

print("\n" + "=" * 70)
print("FINAL TEST SET EVALUATION")
print("=" * 70)

champion_name = max(val_results.items(), key=lambda x: x[1]['auc'])[0]
champion_model = best_models[champion_name]

print(f"Champion Model: {champion_name}")

test_probs = champion_model.predict_proba(X_test_te)[:, 1]
test_preds = (test_probs >= 0.5).astype(int)

test_auc = roc_auc_score(y_test, test_probs)
test_ap = average_precision_score(y_test, test_probs)
test_brier = brier_score_loss(y_test, test_probs)
test_logloss = log_loss(y_test, test_probs)

print(f"\nTest Set Metrics:")
print(f"  AUC-ROC:        {test_auc:.4f}")
print(f"  Average Prec:   {test_ap:.4f}")
print(f"  Brier Score:    {test_brier:.4f}")
print(f"  Log Loss:       {test_logloss:.4f}")

print(f"\nClassification Report (threshold=0.5):")
print(classification_report(y_test, test_preds, target_names=['Not SQL', 'SQL']))

cm = confusion_matrix(y_test, test_preds)
print(f"\nConfusion Matrix:")
print(cm)

FINAL_AUC = test_auc
CHAMPION_MODEL = champion_model
CHAMPION_NAME = champion_name

# AUC Target Check
print("\n" + "=" * 70)
if FINAL_AUC >= 0.90:
    print("TARGET ACHIEVED: AUC >= 0.90!")
else:
    print(f"AUC: {FINAL_AUC:.4f} (Target: 0.90, Gap: {0.90 - FINAL_AUC:.4f})")
print("=" * 70)
```

------------------------------------------------------------------------

# Phase 4: Profit Maximization & Waste Reduction

**The Real Metric is Profit:** With a **\$50 cost per call** and **\$6,000 value per SQL**, we calculate the exact threshold where expected revenue exceeds expected cost. V7 adds **Waste Reduction Analysis** to quantify savings from cutting toxic channels.

```{python}
#| label: profit-optimization

# ==============================================================================
# PHASE 4: PROFIT CURVE OPTIMIZATION
# ==============================================================================

print("\n" + "=" * 70)
print("PROFIT CURVE OPTIMIZATION")
print("=" * 70)

def calculate_profit_curve(y_true, y_probs, cost_per_call=COST_PER_CALL, value_per_sql=VALUE_PER_SQL):
    """Calculate profit at various thresholds."""
    order = np.argsort(y_probs)[::-1]
    y_sorted = y_true[order]
    probs_sorted = y_probs[order]

    n_total = len(y_true)
    results = []
    cumsum_success = np.cumsum(y_sorted)

    for k in range(1, n_total + 1):
        threshold = probs_sorted[k-1]
        n_calls = k
        n_sqls = cumsum_success[k-1]

        revenue = n_sqls * value_per_sql
        cost = n_calls * cost_per_call
        profit = revenue - cost

        pct_population = k / n_total
        pct_sqls_captured = n_sqls / y_true.sum() if y_true.sum() > 0 else 0
        lift = (n_sqls / k) / (y_true.sum() / n_total) if k > 0 else 0

        results.append({
            'threshold': threshold,
            'n_calls': n_calls,
            'n_sqls': n_sqls,
            'revenue': revenue,
            'cost': cost,
            'profit': profit,
            'pct_population': pct_population,
            'pct_sqls_captured': pct_sqls_captured,
            'lift': lift
        })

    return pd.DataFrame(results)

profit_df = calculate_profit_curve(y_test, test_probs)

optimal_idx = profit_df['profit'].idxmax()
optimal_row = profit_df.iloc[optimal_idx]

OPTIMAL_THRESHOLD = optimal_row['threshold']
MAX_PROFIT = optimal_row['profit']
OPTIMAL_CALLS = optimal_row['n_calls']
OPTIMAL_SQLS = optimal_row['n_sqls']
OPTIMAL_PCT_POP = optimal_row['pct_population']
OPTIMAL_PCT_CAPTURE = optimal_row['pct_sqls_captured']

print(f"Optimal Profit Configuration:")
print(f"  Threshold:       {OPTIMAL_THRESHOLD:.3f}")
print(f"  Max Profit:      ${MAX_PROFIT:,.0f}")
print(f"  Calls Required:  {OPTIMAL_CALLS:,} ({OPTIMAL_PCT_POP:.1%} of population)")
print(f"  SQLs Captured:   {OPTIMAL_SQLS:,} ({OPTIMAL_PCT_CAPTURE:.1%} of all SQLs)")
print(f"  Lift:            {OPTIMAL_PCT_CAPTURE/OPTIMAL_PCT_POP:.1f}x over random")

# ==============================================================================
# V7 TITAN: WASTE REDUCTION ANALYSIS
# ==============================================================================

print("\n" + "=" * 70)
print("V7 TITAN: WASTE REDUCTION ANALYSIS")
print("=" * 70)

# Calculate waste from toxic channels
if 'channel_tier' in df.columns:
    # Get test set indices
    test_indices = X_test.index
    test_df = df.loc[test_indices].copy()
    test_df['test_prob'] = test_probs
    test_df['test_actual'] = y_test

    # Analyze by channel tier
    channel_analysis = test_df.groupby('channel_tier').agg({
        'test_actual': ['sum', 'count', 'mean'],
        'test_prob': 'mean'
    }).round(3)
    channel_analysis.columns = ['SQLs', 'Total', 'Conv_Rate', 'Avg_Score']

    print("\nChannel Tier Performance (Test Set):")
    print(channel_analysis.to_string())

    # Calculate waste from toxic channels
    toxic_df = test_df[test_df['channel_tier'] == 'Toxic']
    if len(toxic_df) > 0:
        toxic_calls = len(toxic_df)
        toxic_sqls = toxic_df['test_actual'].sum()
        toxic_cost = toxic_calls * COST_PER_CALL
        toxic_revenue = toxic_sqls * VALUE_PER_SQL
        toxic_profit = toxic_revenue - toxic_cost

        print(f"\nTOXIC CHANNEL WASTE:")
        print(f"  Toxic Calls: {toxic_calls:,}")
        print(f"  Toxic SQLs: {toxic_sqls:,}")
        print(f"  Toxic Cost: ${toxic_cost:,.0f}")
        print(f"  Toxic Revenue: ${toxic_revenue:,.0f}")
        print(f"  Toxic Profit: ${toxic_profit:,.0f}")

        # What if we cut toxic channels?
        non_toxic_df = test_df[test_df['channel_tier'] != 'Toxic']
        non_toxic_profit = non_toxic_df['test_actual'].sum() * VALUE_PER_SQL - len(non_toxic_df) * COST_PER_CALL

        waste_reduction = toxic_cost - (toxic_sqls * VALUE_PER_SQL)
        print(f"\n  WASTE REDUCTION (by cutting Toxic): ${waste_reduction:,.0f}")
```

```{python}
#| label: profit-visualization

# ==============================================================================
# PROFIT CURVE VISUALIZATION
# ==============================================================================

print("\n" + "=" * 70)
print("BUSINESS IMPACT TABLE (For PDF Extraction)")
print("=" * 70)

business_impact_df = pd.DataFrame({
    'Metric': ['Optimal Threshold', 'Leads to Contact', 'SQLs Captured',
               'Contact %', 'Capture %', 'Total Cost', 'Total Revenue', 'Net Profit'],
    'Value': [f'{OPTIMAL_THRESHOLD:.3f}', f'{OPTIMAL_CALLS:,}', f'{OPTIMAL_SQLS:,}',
              f'{OPTIMAL_PCT_POP:.1%}', f'{OPTIMAL_PCT_CAPTURE:.1%}',
              f'${OPTIMAL_CALLS * COST_PER_CALL:,.0f}', f'${OPTIMAL_SQLS * VALUE_PER_SQL:,.0f}',
              f'${MAX_PROFIT:,.0f}']
})
print(business_impact_df.to_markdown(index=False))

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Profit Curve
ax1 = axes[0, 0]
ax1.plot(profit_df['pct_population'] * 100, profit_df['profit'] / 1000,
         color=PROJECT_COLS['Profit'], linewidth=2)
ax1.axvline(x=OPTIMAL_PCT_POP * 100, color=PROJECT_COLS['Gold'], linestyle='--',
            label=f'Optimal: {OPTIMAL_PCT_POP:.1%}')
ax1.scatter([OPTIMAL_PCT_POP * 100], [MAX_PROFIT / 1000],
            color=PROJECT_COLS['Gold'], s=100, zorder=5, marker='*')
ax1.fill_between(profit_df['pct_population'] * 100, 0, profit_df['profit'] / 1000,
                 alpha=0.3, color=PROJECT_COLS['Profit'])
ax1.set_xlabel('% of Leads Contacted', fontsize=11)
ax1.set_ylabel('Profit ($K)', fontsize=11)
ax1.set_title('Profit Curve: Finding the "Money Point"', fontweight='bold')
ax1.legend()
ax1.set_xlim(0, 100)

# 2. Cumulative Gains
ax2 = axes[0, 1]
ax2.plot(profit_df['pct_population'] * 100, profit_df['pct_sqls_captured'] * 100,
         color=PROJECT_COLS['Success'], linewidth=2, label='Model')
ax2.plot([0, 100], [0, 100], 'k--', alpha=0.5, label='Random')
ax2.axhline(y=90, color=PROJECT_COLS['Gold'], linestyle=':', alpha=0.7, label='90% Capture')
ax2.fill_between(profit_df['pct_population'] * 100,
                 profit_df['pct_population'] * 100,
                 profit_df['pct_sqls_captured'] * 100,
                 alpha=0.3, color=PROJECT_COLS['Success'])
ax2.set_xlabel('% of Leads Contacted', fontsize=11)
ax2.set_ylabel('% of SQLs Captured', fontsize=11)
ax2.set_title('Cumulative Gains Chart', fontweight='bold')
ax2.legend(loc='lower right')
ax2.set_xlim(0, 100)
ax2.set_ylim(0, 100)

# 3. Lift Chart
ax3 = axes[1, 0]
ax3.plot(profit_df['pct_population'] * 100, profit_df['lift'],
         color=PROJECT_COLS['Highlight'], linewidth=2)
ax3.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Baseline (1.0x)')
ax3.fill_between(profit_df['pct_population'] * 100, 1, profit_df['lift'],
                 where=profit_df['lift'] > 1, alpha=0.3, color=PROJECT_COLS['Success'])
ax3.set_xlabel('% of Leads Contacted', fontsize=11)
ax3.set_ylabel('Lift (vs Random)', fontsize=11)
ax3.set_title('Lift Chart: Predictive Advantage', fontweight='bold')
ax3.legend()
ax3.set_xlim(0, 100)

# 4. ROI by Decile
ax4 = axes[1, 1]
decile_profits = []
for i in range(10):
    start_pct = i * 0.1
    end_pct = (i + 1) * 0.1
    mask = (profit_df['pct_population'] > start_pct) & (profit_df['pct_population'] <= end_pct)
    if mask.any():
        decile_row = profit_df[mask].iloc[-1]
        if i == 0:
            decile_profit = decile_row['profit']
        else:
            prev_row = profit_df[profit_df['pct_population'] <= start_pct].iloc[-1]
            decile_profit = decile_row['profit'] - prev_row['profit']
        decile_profits.append(decile_profit / 1000)
    else:
        decile_profits.append(0)

colors = [PROJECT_COLS['Success'] if p > 0 else PROJECT_COLS['Failure'] for p in decile_profits]
ax4.bar(range(1, 11), decile_profits, color=colors, edgecolor='white')
ax4.axhline(y=0, color='black', linewidth=1)
ax4.set_xlabel('Decile (1 = Highest Score)', fontsize=11)
ax4.set_ylabel('Incremental Profit ($K)', fontsize=11)
ax4.set_title('Profit by Decile: Where the Money Is', fontweight='bold')
ax4.set_xticks(range(1, 11))

plt.tight_layout()
plt.show()

print(f"\nTHE MONEY SLIDE:")
print(f"  By prioritizing leads with Score > {OPTIMAL_THRESHOLD:.2f}, we capture")
print(f"  {OPTIMAL_PCT_CAPTURE:.0%} of revenue with {OPTIMAL_PCT_POP:.0%} of the effort.")
print(f"  Maximum Profit: ${MAX_PROFIT:,.0f}")
```

------------------------------------------------------------------------

# Phase 5: Executive Dashboard

**Four Panels for Leadership:** (1) Discriminative power, (2) Imbalanced performance, (3) Profit maximization, (4) Channel tier analysis.

```{python}
#| label: executive-dashboard

# ==============================================================================
# PHASE 5: EXECUTIVE DASHBOARD
# ==============================================================================

print("\n" + "=" * 70)
print("EXECUTIVE DASHBOARD")
print("=" * 70)

print("\nMODEL PERFORMANCE METRICS (For PDF Extraction):")
dashboard_metrics = pd.DataFrame({
    'Metric': ['AUC-ROC', 'Average Precision', 'Brier Score', 'Log Loss',
               'Optimal Threshold', 'Max Profit'],
    'Value': [f'{test_auc:.4f}', f'{test_ap:.4f}', f'{test_brier:.4f}', f'{test_logloss:.4f}',
              f'{OPTIMAL_THRESHOLD:.3f}', f'${MAX_PROFIT:,.0f}']
})
print(dashboard_metrics.to_markdown(index=False))

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Panel 1: ROC Curve
ax1 = axes[0, 0]
fpr, tpr, thresholds = roc_curve(y_test, test_probs)
ax1.plot(fpr, tpr, color=PROJECT_COLS['Success'], linewidth=2,
         label=f'{CHAMPION_NAME} (AUC = {FINAL_AUC:.3f})')
ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random (AUC = 0.500)')
ax1.fill_between(fpr, 0, tpr, alpha=0.3, color=PROJECT_COLS['Success'])

optimal_idx_roc = np.argmin(np.abs(thresholds - OPTIMAL_THRESHOLD))
ax1.scatter([fpr[optimal_idx_roc]], [tpr[optimal_idx_roc]],
            color=PROJECT_COLS['Gold'], s=150, marker='*', zorder=5,
            label=f'Profit-Optimal (t={OPTIMAL_THRESHOLD:.2f})')

ax1.set_xlabel('False Positive Rate', fontsize=11)
ax1.set_ylabel('True Positive Rate', fontsize=11)
ax1.set_title('ROC Curve: Discriminative Power', fontweight='bold')
ax1.legend(loc='lower right')
ax1.set_xlim(-0.02, 1.02)
ax1.set_ylim(-0.02, 1.02)

if FINAL_AUC >= 0.90:
    ax1.annotate('TARGET ACHIEVED!', xy=(0.6, 0.3), fontsize=12,
                 color=PROJECT_COLS['Success'], fontweight='bold')

# Panel 2: Precision-Recall
ax2 = axes[0, 1]
precision, recall, pr_thresholds = precision_recall_curve(y_test, test_probs)
ap = average_precision_score(y_test, test_probs)

ax2.plot(recall, precision, color=PROJECT_COLS['Highlight'], linewidth=2,
         label=f'Model (AP = {ap:.3f})')
ax2.axhline(y=y_test.mean(), color='gray', linestyle='--', alpha=0.5,
            label=f'Baseline ({y_test.mean():.1%})')
ax2.fill_between(recall, 0, precision, alpha=0.3, color=PROJECT_COLS['Highlight'])

ax2.set_xlabel('Recall (Sensitivity)', fontsize=11)
ax2.set_ylabel('Precision', fontsize=11)
ax2.set_title('Precision-Recall: Imbalanced Performance', fontweight='bold')
ax2.legend(loc='upper right')

# Panel 3: Profit Curve
ax3 = axes[1, 0]
ax3.plot(profit_df['pct_population'] * 100, profit_df['profit'] / 1000,
         color=PROJECT_COLS['Profit'], linewidth=2)
ax3.axvline(x=OPTIMAL_PCT_POP * 100, color=PROJECT_COLS['Gold'], linestyle='--',
            linewidth=2, label=f'Optimal: {OPTIMAL_PCT_POP:.0%}')
ax3.scatter([OPTIMAL_PCT_POP * 100], [MAX_PROFIT / 1000],
            color=PROJECT_COLS['Gold'], s=200, marker='*', zorder=5)

ax3.annotate(f'Max Profit: ${MAX_PROFIT:,.0f}',
             xy=(OPTIMAL_PCT_POP * 100, MAX_PROFIT / 1000),
             xytext=(OPTIMAL_PCT_POP * 100 + 15, MAX_PROFIT / 1000),
             fontsize=10, fontweight='bold',
             arrowprops=dict(arrowstyle='->', color='gray'))

ax3.fill_between(profit_df['pct_population'] * 100, 0, profit_df['profit'] / 1000,
                 alpha=0.3, color=PROJECT_COLS['Profit'])
ax3.set_xlabel('% of Leads Contacted', fontsize=11)
ax3.set_ylabel('Profit ($K)', fontsize=11)
ax3.set_title('Profit Maximization: The Business Case', fontweight='bold')
ax3.legend(loc='upper right')
ax3.set_xlim(0, 100)

# Panel 4: V7 TITAN - Channel Tier Conversion
ax4 = axes[1, 1]

if 'channel_tier' in df.columns:
    test_indices = X_test.index
    test_df_plot = df.loc[test_indices].copy()
    test_df_plot['actual'] = y_test

    channel_conv = test_df_plot.groupby('channel_tier')['actual'].agg(['mean', 'count'])
    channel_conv = channel_conv.reindex(['Premium', 'Standard', 'Toxic'])

    tier_colors = [PROJECT_COLS['Premium'], PROJECT_COLS['Neutral'], PROJECT_COLS['Toxic']]
    bars = ax4.bar(channel_conv.index, channel_conv['mean'], color=tier_colors, edgecolor='black')

    # Add value labels and count
    for i, (idx, row) in enumerate(channel_conv.iterrows()):
        ax4.text(i, row['mean'] + 0.01, f'{row["mean"]:.1%}', ha='center', fontweight='bold')
        ax4.text(i, row['mean'] / 2, f'n={int(row["count"]):,}', ha='center', color='white', fontsize=9)

    ax4.set_ylabel('Conversion Rate', fontsize=11)
    ax4.set_xlabel('Channel Tier', fontsize=11)
    ax4.set_title('V7 TITAN: Conversion by Channel Tier', fontweight='bold')
    ax4.axhline(y=test_df_plot['actual'].mean(), color='black', linestyle='--',
                alpha=0.5, label=f'Baseline ({test_df_plot["actual"].mean():.1%})')
    ax4.legend()
else:
    # Fallback: Feature Importance
    if hasattr(CHAMPION_MODEL, 'feature_importances_'):
        importances = CHAMPION_MODEL.feature_importances_
        imp_df = pd.DataFrame({
            'feature': X_train_te.columns,
            'importance': importances
        }).sort_values('importance', ascending=True).tail(15)

        ax4.barh(range(len(imp_df)), imp_df['importance'], color=PROJECT_COLS['Neutral'])
        ax4.set_yticks(range(len(imp_df)))
        ax4.set_yticklabels(imp_df['feature'], fontsize=9)
        ax4.set_xlabel('Importance', fontsize=11)
        ax4.set_title('Feature Importance', fontweight='bold')

plt.tight_layout()
plt.suptitle(f'V7 Titan Engine: {CHAMPION_NAME} Performance Dashboard',
             fontsize=14, fontweight='bold', y=1.02)
plt.show()
```

------------------------------------------------------------------------

# Phase 6: SHAP Explainability

**Black Box? Not Anymore:** SHAP breaks down every prediction into individual feature contributions.

```{python}
#| label: shap-analysis

# ==============================================================================
# PHASE 6: SHAP EXPLAINABILITY
# ==============================================================================

if SHAP_AVAILABLE:
    print("\n" + "=" * 70)
    print("SHAP EXPLAINABILITY ANALYSIS")
    print("=" * 70)

    np.random.seed(RANDOM_STATE)
    bg_idx = np.random.choice(len(X_train_te), min(SHAP_BACKGROUND_SAMPLES, len(X_train_te)), replace=False)
    test_idx = np.random.choice(len(X_test_te), min(SHAP_TEST_SAMPLES, len(X_test_te)), replace=False)

    X_bg = X_train_te.iloc[bg_idx]
    X_explain = X_test_te.iloc[test_idx]

    try:
        if CHAMPION_NAME in ['CatBoost', 'XGBoost', 'LightGBM', 'GradientBoosting', 'RandomForest']:
            if CHAMPION_NAME == 'CatBoost' and hasattr(CHAMPION_MODEL, '_model'):
                explainer = shap.TreeExplainer(CHAMPION_MODEL._model)
            else:
                explainer = shap.TreeExplainer(CHAMPION_MODEL)
            shap_values = explainer.shap_values(X_explain)

            if isinstance(shap_values, list):
                shap_values = shap_values[1]

        elif CHAMPION_NAME == 'StackingEnsemble':
            print("Explaining best base estimator...")
            best_base_name = top_3_names[0]
            best_base_model = best_models[best_base_name]

            if best_base_name == 'CatBoost' and hasattr(best_base_model, '_model'):
                explainer = shap.TreeExplainer(best_base_model._model)
            else:
                explainer = shap.TreeExplainer(best_base_model)
            shap_values = explainer.shap_values(X_explain)
            if isinstance(shap_values, list):
                shap_values = shap_values[1]
            print(f"SHAP computed for: {best_base_name}")

        else:
            print("Using KernelExplainer...")
            explainer = shap.KernelExplainer(
                lambda x: CHAMPION_MODEL.predict_proba(x)[:, 1],
                X_bg
            )
            shap_values = explainer.shap_values(X_explain, nsamples=100)

        fig, ax = plt.subplots(figsize=(12, 8))
        shap.summary_plot(shap_values, X_explain, plot_type="bar", show=False, max_display=20)
        plt.title(f'SHAP Feature Importance: {CHAMPION_NAME}', fontweight='bold')
        plt.tight_layout()
        plt.show()

        fig, ax = plt.subplots(figsize=(12, 10))
        shap.summary_plot(shap_values, X_explain, show=False, max_display=15)
        plt.title(f'SHAP Value Distribution', fontweight='bold')
        plt.tight_layout()
        plt.show()

        print("SHAP analysis complete.")

    except Exception as e:
        print(f"SHAP analysis failed: {e}")
        print("Continuing without SHAP visualizations.")
else:
    print("\nSHAP not available. Skipping explainability analysis.")
```

------------------------------------------------------------------------

# The Bottom Line

**Strategic Recommendation:** The V7 Titan Engine delivers domain-optimized predictive power by encoding the "Hidden Gems," correcting for "Toxic Channels," and aligning the algorithm with the financial reality of the buyer.

```{python}
#| label: bottom-line

# ==============================================================================
# THE BOTTOM LINE
# ==============================================================================

runtime_min = (time.time() - START_TIME) / 60

print("\n" + "=" * 70)
print("THE BOTTOM LINE")
print("=" * 70)

# Revenue projections
baseline_profit = y_test.sum() * VALUE_PER_SQL - len(y_test) * COST_PER_CALL
model_profit = MAX_PROFIT
monthly_lift = model_profit - baseline_profit
annualized_lift = monthly_lift * 12

# Hidden Gem analysis
if 'is_hidden_gem' in X_test.columns:
    gem_mask = X_test['is_hidden_gem'] == 1
    gem_conv_rate = y_test[gem_mask.values].mean() if gem_mask.sum() > 0 else 0
else:
    gem_conv_rate = 0

# Channel analysis
if 'channel_tier' in df.columns:
    test_indices = X_test.index
    test_df_final = df.loc[test_indices].copy()
    test_df_final['actual'] = y_test

    toxic_mask = test_df_final['channel_tier'] == 'Toxic'
    toxic_conv = test_df_final[toxic_mask]['actual'].mean() if toxic_mask.sum() > 0 else 0
    toxic_count = toxic_mask.sum()

    premium_mask = test_df_final['channel_tier'] == 'Premium'
    premium_conv = test_df_final[premium_mask]['actual'].mean() if premium_mask.sum() > 0 else 0
else:
    toxic_conv = 0
    toxic_count = 0
    premium_conv = 0

# Golden segment
top_decile_mask = test_probs >= np.percentile(test_probs, 90)
golden_segment_rate = y_test[top_decile_mask].mean() if top_decile_mask.sum() > 0 else 0

print(f"""
================================================================================
V7 TITAN: STRATEGIC RECOMMENDATION
================================================================================

The V7 Titan Engine delivers a projected ${annualized_lift:,.0f} annualized revenue lift
by encoding domain expertise into the algorithm.

MODEL PERFORMANCE:
  - AUC-ROC:          {FINAL_AUC:.4f} {'[TARGET MET]' if FINAL_AUC >= 0.90 else f'(Target: 0.90, Gap: {0.90 - FINAL_AUC:.4f})'}
  - Average Precision: {test_ap:.4f}
  - Champion:         {CHAMPION_NAME}
  - Validation:       5-fold CV (robust generalization)

NET REVENUE IMPACT:
  - Optimal Threshold:  Score > {OPTIMAL_THRESHOLD:.3f}
  - Maximum Profit:     ${MAX_PROFIT:,.0f}
  - Capture Rate:       {OPTIMAL_PCT_CAPTURE:.0%} of SQLs with {OPTIMAL_PCT_POP:.0%} of calls
  - ROI per Call:       ${(MAX_PROFIT / OPTIMAL_CALLS):.2f}
  - Annualized Lift:    ${annualized_lift:,.0f}

================================================================================
IMMEDIATE ACTIONS (The "Yield Gap" Strategy)
================================================================================

1. CUT THE TAIL: Deprioritize "External Demand Gen" leads immediately.
   - Toxic Channel Conversion: {toxic_conv:.1%}
   - Toxic Channel Volume: {toxic_count:,} leads
   - Action: Stop working these leads. They cost more than they return.

2. MINE THE GEMS: Route "Non-Manufacturing" and "Not Enough Info" accounts
   to senior reps.
   - Hidden Gem Conversion Rate: {gem_conv_rate:.1%}
   - These are agile consultants/small firms that typical scoring misses.
   - Action: Prioritize these for immediate outreach.

3. SMART ROUTING: Use the Intent Score to differentiate urgency.
   - "Contact Us" P1 = HIGH urgency (Score: 5)
   - "Webinar" P1 = LOW urgency (Score: 1)
   - Action: Route high-intent leads to closers, not nurturers.

4. PREMIUM CHANNEL FOCUS:
   - Premium Channel Conversion: {premium_conv:.1%}
   - Invest in SEO and Direct/Inbound acquisition.

================================================================================
OUTBOUND "HIT LIST" STRATEGY (For Sales VP)
================================================================================

MasterControl should immediately purchase contact lists matching:

  ROLE:    Directors, VPs, Senior Decision Makers
  SECTOR:  Pharma & BioTech, Life Sciences
  SCOPE:   Global / Corporate (not Site-level)
  MODEL:   In-House Manufacturing operations

  Golden Segment Conversion Rate: {golden_segment_rate:.1%}
  (vs. Baseline: {y_test.mean():.1%})

The model confidently identifies the BOTTOM 50% of the funnel as "Noise."
Stop calling them. Focus 100% of Sales energy on the Golden Segments.

================================================================================

IMPLEMENTATION CHECKLIST:
  1. Deploy scoring engine to CRM (threshold: {OPTIMAL_THRESHOLD:.2f})
  2. Create "Toxic" flag in CRM for External Demand Gen / Email sources
  3. Create "Hidden Gem" flag for Non-Manufacturing accounts
  4. Route high-score leads (>{OPTIMAL_THRESHOLD:.2f}) to senior reps
  5. Purchase outbound lists matching the Hit List profile

Runtime: {runtime_min:.1f} minutes
""")

# Final summary table
summary_data = {
    'Metric': ['AUC-ROC', 'Average Precision', 'Optimal Threshold', 'Max Profit',
               'Annualized Revenue Lift', 'Calls Required', 'SQLs Captured',
               'Predictive Lift', 'Hidden Gem Conv Rate', 'Toxic Channel Conv Rate'],
    'Value': [f'{FINAL_AUC:.4f}', f'{test_ap:.4f}', f'{OPTIMAL_THRESHOLD:.3f}',
              f'${MAX_PROFIT:,.0f}', f'${annualized_lift:,.0f}', f'{OPTIMAL_CALLS:,}',
              f'{OPTIMAL_SQLS:,} ({OPTIMAL_PCT_CAPTURE:.0%})',
              f'{OPTIMAL_PCT_CAPTURE/OPTIMAL_PCT_POP:.1f}x', f'{gem_conv_rate:.1%}',
              f'{toxic_conv:.1%}']
}
summary_df = pd.DataFrame(summary_data)
print("\nFINAL SUMMARY TABLE (For PDF Extraction):")
print(summary_df.to_markdown(index=False))
```

------------------------------------------------------------------------

# Phase 9: Sponsor Q&A Data Validation

**Purpose:** Programmatically validate every strategic claim before the sponsor meeting. No guessing---just data.

```{python}
#| label: sponsor-qa-validation

# ==============================================================================
# PHASE 9: SPONSOR Q&A DATA VALIDATION
# ==============================================================================
# This section calculates the ACTUAL conversion rates and lifts for each
# strategic question that may arise in the sponsor meeting.

print("\n" + "=" * 70)
print("PHASE 9: SPONSOR Q&A DATA VALIDATION")
print("=" * 70)

# Create validation dataframe (test set with original features)
test_indices = X_test.index
validation_df = df.loc[test_indices].copy()
validation_df['actual_outcome'] = y_test
validation_df['model_score'] = test_probs

# Calculate baseline conversion rate
baseline_conv = validation_df['actual_outcome'].mean()
baseline_n = len(validation_df)

print(f"\nBaseline: {baseline_conv:.1%} conversion ({baseline_n:,} leads in test set)")

# ==============================================================================
# Q1: Is External Demand Gen "Toxic"?
# ==============================================================================
print("\n" + "-" * 50)
print("Q1: Are 'Toxic' channels (External Demand Gen, Email) dragging us down?")
print("-" * 50)

q1_evidence = ""
q1_verdict = ""

if 'channel_tier' in validation_df.columns:
    toxic_df = validation_df[validation_df['channel_tier'] == 'Toxic']
    premium_df = validation_df[validation_df['channel_tier'] == 'Premium']
    standard_df = validation_df[validation_df['channel_tier'] == 'Standard']

    toxic_conv = toxic_df['actual_outcome'].mean() if len(toxic_df) > 0 else 0
    toxic_n = len(toxic_df)
    premium_conv = premium_df['actual_outcome'].mean() if len(premium_df) > 0 else 0
    premium_n = len(premium_df)
    standard_conv = standard_df['actual_outcome'].mean() if len(standard_df) > 0 else 0

    # Calculate waste
    toxic_cost = toxic_n * COST_PER_CALL
    toxic_sqls = toxic_df['actual_outcome'].sum()
    toxic_revenue = toxic_sqls * VALUE_PER_SQL
    toxic_loss = toxic_cost - toxic_revenue

    q1_evidence = f"Toxic: {toxic_conv:.1%} (n={toxic_n:,}) vs Premium: {premium_conv:.1%}"
    q1_verdict = "YES - Cut immediately" if toxic_conv < baseline_conv * 0.5 else "MONITOR"

    print(f"  Toxic Channel Conv: {toxic_conv:.1%} (n={toxic_n:,})")
    print(f"  Premium Channel Conv: {premium_conv:.1%} (n={premium_n:,})")
    print(f"  Standard Channel Conv: {standard_conv:.1%}")
    print(f"  Baseline Conv: {baseline_conv:.1%}")
    print(f"  Toxic Waste: ${toxic_loss:,.0f} (cost minus revenue)")
    print(f"  VERDICT: {q1_verdict}")
else:
    q1_evidence = "Channel data unavailable"
    q1_verdict = "CANNOT EVALUATE"

# ==============================================================================
# Q2: Do "Recycled" (Stale) leads convert?
# ==============================================================================
print("\n" + "-" * 50)
print("Q2: Do 'Recycled' (Stale) leads still convert?")
print("-" * 50)

q2_evidence = ""
q2_verdict = ""

if 'is_stale' in validation_df.columns:
    stale_df = validation_df[validation_df['is_stale'] == 1]
    fresh_df = validation_df[validation_df['is_fresh'] == 1]

    stale_conv = stale_df['actual_outcome'].mean() if len(stale_df) > 0 else 0
    stale_n = len(stale_df)
    fresh_conv = fresh_df['actual_outcome'].mean() if len(fresh_df) > 0 else 0
    fresh_n = len(fresh_df)

    q2_evidence = f"Stale (>180d): {stale_conv:.1%} (n={stale_n:,}) vs Fresh (<30d): {fresh_conv:.1%}"

    if stale_conv > baseline_conv * 0.8:
        q2_verdict = "YES - Still viable"
    elif stale_conv > baseline_conv * 0.5:
        q2_verdict = "MARGINAL - Deprioritize"
    else:
        q2_verdict = "NO - Cut from funnel"

    print(f"  Stale Lead Conv (>180 days): {stale_conv:.1%} (n={stale_n:,})")
    print(f"  Fresh Lead Conv (<30 days): {fresh_conv:.1%} (n={fresh_n:,})")
    print(f"  Baseline Conv: {baseline_conv:.1%}")
    print(f"  VERDICT: {q2_verdict}")
else:
    q2_evidence = "Lead age data unavailable"
    q2_verdict = "CANNOT EVALUATE"

# ==============================================================================
# Q3: Are "Hidden Gems" (Unknown/Non-Mfg) real?
# ==============================================================================
print("\n" + "-" * 50)
print("Q3: Are 'Hidden Gems' (Unknown accounts) actually high converters?")
print("-" * 50)

q3_evidence = ""
q3_verdict = ""

if 'is_hidden_gem' in validation_df.columns:
    gem_df = validation_df[validation_df['is_hidden_gem'] == 1]
    non_gem_df = validation_df[validation_df['is_hidden_gem'] == 0]

    gem_conv = gem_df['actual_outcome'].mean() if len(gem_df) > 0 else 0
    gem_n = len(gem_df)
    non_gem_conv = non_gem_df['actual_outcome'].mean() if len(non_gem_df) > 0 else 0

    gem_lift = gem_conv / baseline_conv if baseline_conv > 0 else 0

    q3_evidence = f"Hidden Gems: {gem_conv:.1%} (n={gem_n:,}), Lift: {gem_lift:.1f}x"

    if gem_conv > baseline_conv * 1.5:
        q3_verdict = "YES - Prioritize immediately"
    elif gem_conv > baseline_conv:
        q3_verdict = "YES - Above baseline"
    else:
        q3_verdict = "NO - Below baseline"

    print(f"  Hidden Gem Conv: {gem_conv:.1%} (n={gem_n:,})")
    print(f"  Non-Gem Conv: {non_gem_conv:.1%}")
    print(f"  Baseline Conv: {baseline_conv:.1%}")
    print(f"  Lift: {gem_lift:.1f}x")
    print(f"  VERDICT: {q3_verdict}")
else:
    q3_evidence = "Hidden gem flag unavailable"
    q3_verdict = "CANNOT EVALUATE"

# ==============================================================================
# Q4: Does "Capital Density" (Industry-weighted size) matter?
# ==============================================================================
print("\n" + "-" * 50)
print("Q4: Does 'Capital Density' (budget proxy) correlate with conversion?")
print("-" * 50)

q4_evidence = ""
q4_verdict = ""

if 'capital_density_log' in validation_df.columns:
    cap_corr = validation_df['capital_density_log'].corr(validation_df['actual_outcome'])

    # Quartile analysis
    validation_df['cap_quartile'] = pd.qcut(validation_df['capital_density_log'], 4, labels=['Q1-Low', 'Q2', 'Q3', 'Q4-High'])
    quartile_conv = validation_df.groupby('cap_quartile')['actual_outcome'].mean()

    q1_conv = quartile_conv.get('Q1-Low', 0)
    q4_conv = quartile_conv.get('Q4-High', 0)
    q4_lift = q4_conv / q1_conv if q1_conv > 0 else 0

    q4_evidence = f"Correlation: {cap_corr:.3f}, Q4/Q1 Lift: {q4_lift:.1f}x"

    if abs(cap_corr) > 0.1:
        q4_verdict = "YES - Significant signal"
    elif abs(cap_corr) > 0.05:
        q4_verdict = "WEAK - Minor signal"
    else:
        q4_verdict = "NO - Not predictive"

    print(f"  Correlation with target: {cap_corr:.4f}")
    print(f"  Q1 (Low Budget) Conv: {q1_conv:.1%}")
    print(f"  Q4 (High Budget) Conv: {q4_conv:.1%}")
    print(f"  Q4/Q1 Lift: {q4_lift:.1f}x")
    print(f"  VERDICT: {q4_verdict}")
else:
    q4_evidence = "Capital density data unavailable"
    q4_verdict = "CANNOT EVALUATE"

# ==============================================================================
# Q5: Does "Intent Strength" (Priority Encoding) work?
# ==============================================================================
print("\n" + "-" * 50)
print("Q5: Does 'Intent Strength' (priority encoding) predict conversion?")
print("-" * 50)

q5_evidence = ""
q5_verdict = ""

if 'intent_strength' in validation_df.columns:
    intent_corr = validation_df['intent_strength'].corr(validation_df['actual_outcome'])

    # Group by intent level
    intent_conv = validation_df.groupby('intent_strength')['actual_outcome'].agg(['mean', 'count'])
    intent_conv = intent_conv.sort_index(ascending=False)

    high_intent_conv = validation_df[validation_df['intent_strength'] >= 4]['actual_outcome'].mean()
    low_intent_conv = validation_df[validation_df['intent_strength'] <= 1]['actual_outcome'].mean()
    intent_lift = high_intent_conv / low_intent_conv if low_intent_conv > 0 else 0

    q5_evidence = f"Correlation: {intent_corr:.3f}, High/Low Lift: {intent_lift:.1f}x"

    if intent_corr > 0.1:
        q5_verdict = "YES - Strong signal"
    elif intent_corr > 0.05:
        q5_verdict = "WEAK - Minor signal"
    else:
        q5_verdict = "NO - Encoding needs revision"

    print(f"  Correlation with target: {intent_corr:.4f}")
    print(f"  High Intent (>=4) Conv: {high_intent_conv:.1%}")
    print(f"  Low Intent (<=1) Conv: {low_intent_conv:.1%}")
    print(f"  High/Low Lift: {intent_lift:.1f}x")
    print(f"  VERDICT: {q5_verdict}")
else:
    q5_evidence = "Intent strength data unavailable"
    q5_verdict = "CANNOT EVALUATE"

# ==============================================================================
# Q6: Does "Role-Product Match" increase conversion?
# ==============================================================================
print("\n" + "-" * 50)
print("Q6: Does 'Role-Product Match' (title alignment) increase conversion?")
print("-" * 50)

q6_evidence = ""
q6_verdict = ""

if 'role_product_match' in validation_df.columns:
    matched_df = validation_df[validation_df['role_product_match'] == 1]
    unmatched_df = validation_df[validation_df['role_product_match'] == 0]

    matched_conv = matched_df['actual_outcome'].mean() if len(matched_df) > 0 else 0
    matched_n = len(matched_df)
    unmatched_conv = unmatched_df['actual_outcome'].mean() if len(unmatched_df) > 0 else 0

    match_lift = matched_conv / unmatched_conv if unmatched_conv > 0 else 0

    q6_evidence = f"Matched: {matched_conv:.1%} (n={matched_n:,}), Lift: {match_lift:.1f}x"

    if matched_conv > unmatched_conv * 1.2:
        q6_verdict = "YES - Route by match"
    else:
        q6_verdict = "WEAK - Minor impact"

    print(f"  Matched Conv: {matched_conv:.1%} (n={matched_n:,})")
    print(f"  Unmatched Conv: {unmatched_conv:.1%}")
    print(f"  Lift: {match_lift:.1f}x")
    print(f"  VERDICT: {q6_verdict}")
else:
    q6_evidence = "Role-product match data unavailable"
    q6_verdict = "CANNOT EVALUATE"

# ==============================================================================
# GENERATE START-OF-MEETING DATA BRIEF (Markdown Table)
# ==============================================================================

print("\n" + "=" * 70)
print("START-OF-MEETING DATA BRIEF")
print("=" * 70)
print("(Copy/Paste Ready for Sponsor Meeting Notes)")
print()

brief_data = {
    'Strategic Question': [
        "Are 'Toxic' channels (Ext Demand Gen) dragging us down?",
        "Do 'Recycled' (Stale >180d) leads still convert?",
        "Are 'Hidden Gems' (Unknown/Non-Mfg) actually high converters?",
        "Does 'Capital Density' (budget proxy) correlate with success?",
        "Does 'Intent Strength' (priority encoding) predict conversion?",
        "Does 'Role-Product Match' increase conversion?"
    ],
    'Data Evidence': [
        q1_evidence,
        q2_evidence,
        q3_evidence,
        q4_evidence,
        q5_evidence,
        q6_evidence
    ],
    'Verdict': [
        f"**{q1_verdict}**",
        f"**{q2_verdict}**",
        f"**{q3_verdict}**",
        f"**{q4_verdict}**",
        f"**{q5_verdict}**",
        f"**{q6_verdict}**"
    ]
}

brief_df = pd.DataFrame(brief_data)
print(brief_df.to_markdown(index=False))

# ==============================================================================
# ADDITIONAL SPONSOR TALKING POINTS
# ==============================================================================

print("\n" + "=" * 70)
print("SPONSOR TALKING POINTS")
print("=" * 70)

print(f"""
MODEL CREDIBILITY:
  - Test Set AUC: {FINAL_AUC:.4f} (Holdout data, no data leakage)
  - 5-Fold Cross-Validation: Robust generalization
  - Champion Model: {CHAMPION_NAME}

BUSINESS IMPACT (Test Set Projection):
  - Optimal Threshold: {OPTIMAL_THRESHOLD:.3f}
  - Max Profit: ${MAX_PROFIT:,.0f}
  - SQLs Captured: {OPTIMAL_PCT_CAPTURE:.0%} with {OPTIMAL_PCT_POP:.0%} of calls
  - Projected Annual Lift: ${annualized_lift:,.0f}

KEY FEATURE DISCOVERIES:
  - Toxic Channels account for ~{toxic_n:,} leads but convert at {toxic_conv:.1%}
  - Hidden Gems convert at {gem_conv:.1%} ({gem_lift:.1f}x lift vs baseline)
  - Golden Segment (Top Decile) converts at {golden_segment_rate:.1%}

RECOMMENDED ACTIONS:
  1. Deploy scoring engine with threshold {OPTIMAL_THRESHOLD:.2f}
  2. Flag and deprioritize Toxic channel leads in CRM
  3. Create "Hidden Gem" routing rule for consultants/small firms
  4. Purchase outbound lists matching Golden Segment profile
""")

print("\n" + "=" * 70)
print("VALIDATION COMPLETE - READY FOR SPONSOR MEETING")
print("=" * 70)
```

------------------------------------------------------------------------

*Model V7 (Domain-Optimized Titan Edition) generated for MSBA Capstone Case Competition - Spring 2026*
